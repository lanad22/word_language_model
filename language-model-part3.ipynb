{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_2 import main, Config\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize_nhid_values = [200, 400, 600]  # Example values\n",
    "nlayers_values = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running experiment on LSTM model only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment for: emsize_nhid:  200, nlayers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phuongdo/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 119.88 | loss  7.51 | ppl  1825.08\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 119.79 | loss  6.63 | ppl   759.28\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 124.85 | loss  6.26 | ppl   525.20\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 130.36 | loss  6.12 | ppl   456.30\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 131.05 | loss  6.00 | ppl   402.27\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 137.95 | loss  5.94 | ppl   378.15\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 133.98 | loss  5.84 | ppl   344.20\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 132.68 | loss  5.86 | ppl   350.49\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 132.19 | loss  5.71 | ppl   301.58\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 131.87 | loss  5.68 | ppl   293.92\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 132.00 | loss  5.58 | ppl   265.19\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 132.38 | loss  5.60 | ppl   269.69\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 132.92 | loss  5.59 | ppl   268.17\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 133.01 | loss  5.48 | ppl   240.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 406.64s | valid loss  5.50 | valid ppl   245.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 134.95 | loss  5.48 | ppl   241.00\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 133.06 | loss  5.46 | ppl   234.34\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 133.74 | loss  5.28 | ppl   196.92\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 132.72 | loss  5.30 | ppl   199.81\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 132.93 | loss  5.28 | ppl   195.61\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 133.29 | loss  5.26 | ppl   192.22\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 132.97 | loss  5.26 | ppl   192.43\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 133.60 | loss  5.32 | ppl   203.46\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 133.68 | loss  5.19 | ppl   179.25\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 134.19 | loss  5.20 | ppl   181.60\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 133.42 | loss  5.11 | ppl   165.45\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 134.55 | loss  5.15 | ppl   172.01\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 135.41 | loss  5.16 | ppl   173.35\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 133.98 | loss  5.08 | ppl   160.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 416.90s | valid loss  5.23 | valid ppl   186.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 136.68 | loss  5.12 | ppl   168.04\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 133.64 | loss  5.14 | ppl   171.12\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 134.00 | loss  4.96 | ppl   142.08\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 134.03 | loss  5.00 | ppl   148.28\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 133.69 | loss  4.99 | ppl   147.65\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 133.81 | loss  4.98 | ppl   145.89\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 133.44 | loss  5.01 | ppl   150.17\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 134.04 | loss  5.08 | ppl   160.17\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 134.71 | loss  4.95 | ppl   141.24\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 134.33 | loss  4.98 | ppl   145.88\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 134.07 | loss  4.89 | ppl   132.51\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 134.08 | loss  4.93 | ppl   137.93\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 134.15 | loss  4.94 | ppl   139.46\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 134.50 | loss  4.87 | ppl   129.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 418.06s | valid loss  5.16 | valid ppl   173.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 138.37 | loss  4.93 | ppl   137.84\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 4797.62 | loss  4.95 | ppl   141.04\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 119.11 | loss  4.76 | ppl   117.22\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 4844.17 | loss  4.82 | ppl   123.80\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 119.49 | loss  4.82 | ppl   123.42\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 2708.77 | loss  4.82 | ppl   123.56\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 119.86 | loss  4.85 | ppl   128.27\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 118.93 | loss  4.91 | ppl   136.27\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 119.57 | loss  4.80 | ppl   121.28\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 119.90 | loss  4.83 | ppl   125.29\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 124.65 | loss  4.73 | ppl   113.50\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 129.64 | loss  4.77 | ppl   118.28\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 129.34 | loss  4.79 | ppl   120.49\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 129.88 | loss  4.72 | ppl   112.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 2784.81s | valid loss  5.06 | valid ppl   158.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 131.23 | loss  4.78 | ppl   118.99\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 131.32 | loss  4.81 | ppl   122.38\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 131.21 | loss  4.62 | ppl   101.89\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 131.69 | loss  4.68 | ppl   107.75\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 133.49 | loss  4.68 | ppl   107.96\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 132.27 | loss  4.69 | ppl   108.78\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 131.39 | loss  4.73 | ppl   112.98\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 132.76 | loss  4.80 | ppl   120.93\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 135.78 | loss  4.68 | ppl   108.15\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 132.76 | loss  4.72 | ppl   111.86\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 131.79 | loss  4.61 | ppl   100.88\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 132.67 | loss  4.66 | ppl   105.75\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 132.16 | loss  4.68 | ppl   107.44\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 133.00 | loss  4.61 | ppl   100.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 705.73s | valid loss  5.04 | valid ppl   154.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.97 | test ppl   143.77\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  200, nlayers: 3\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 4645.93 | loss  7.74 | ppl  2290.68\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 144.80 | loss  7.23 | ppl  1373.85\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 4696.55 | loss  6.80 | ppl   895.38\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 147.05 | loss  6.59 | ppl   728.21\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 5320.50 | loss  6.44 | ppl   623.72\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 145.31 | loss  6.30 | ppl   542.99\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 145.41 | loss  6.16 | ppl   472.29\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 142.21 | loss  6.13 | ppl   457.44\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 147.13 | loss  5.99 | ppl   399.20\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 154.18 | loss  5.95 | ppl   384.62\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 157.65 | loss  5.82 | ppl   338.56\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 160.54 | loss  5.83 | ppl   338.97\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 163.04 | loss  5.80 | ppl   330.79\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 162.76 | loss  5.68 | ppl   293.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 3319.14s | valid loss  5.67 | valid ppl   290.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 165.39 | loss  5.67 | ppl   290.14\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 385.59 | loss  5.66 | ppl   287.17\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 147.27 | loss  5.51 | ppl   246.68\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 149.23 | loss  5.54 | ppl   253.53\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 160.06 | loss  5.50 | ppl   243.86\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 161.21 | loss  5.48 | ppl   240.13\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 160.80 | loss  5.46 | ppl   234.99\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 161.15 | loss  5.51 | ppl   246.98\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 159.77 | loss  5.39 | ppl   220.13\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 160.29 | loss  5.40 | ppl   220.86\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 160.11 | loss  5.30 | ppl   200.18\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 160.08 | loss  5.34 | ppl   208.17\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 160.09 | loss  5.35 | ppl   211.19\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 160.75 | loss  5.25 | ppl   191.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 540.59s | valid loss  5.37 | valid ppl   215.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 162.20 | loss  5.31 | ppl   201.79\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 160.00 | loss  5.32 | ppl   204.55\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 160.58 | loss  5.15 | ppl   172.73\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 160.51 | loss  5.20 | ppl   181.42\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 161.43 | loss  5.18 | ppl   178.57\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 161.70 | loss  5.18 | ppl   178.35\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 161.39 | loss  5.19 | ppl   178.97\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 163.42 | loss  5.25 | ppl   190.65\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 163.80 | loss  5.13 | ppl   169.38\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 171.97 | loss  5.15 | ppl   173.16\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 166.35 | loss  5.06 | ppl   158.31\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 163.59 | loss  5.10 | ppl   164.38\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 161.42 | loss  5.12 | ppl   167.03\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 161.80 | loss  5.04 | ppl   154.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 506.88s | valid loss  5.22 | valid ppl   185.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 165.20 | loss  5.11 | ppl   165.09\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 161.86 | loss  5.13 | ppl   169.18\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 161.38 | loss  4.95 | ppl   141.32\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 161.99 | loss  5.00 | ppl   149.03\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 166.68 | loss  5.00 | ppl   148.56\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 162.23 | loss  5.00 | ppl   148.62\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 162.62 | loss  5.03 | ppl   152.91\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 164.35 | loss  5.10 | ppl   163.48\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 162.68 | loss  4.98 | ppl   145.54\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 162.69 | loss  5.00 | ppl   148.64\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 162.78 | loss  4.91 | ppl   136.23\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 163.29 | loss  4.95 | ppl   140.82\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 163.06 | loss  4.97 | ppl   144.00\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 163.66 | loss  4.90 | ppl   134.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 508.29s | valid loss  5.17 | valid ppl   175.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 163.84 | loss  4.97 | ppl   144.53\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 167.72 | loss  5.00 | ppl   148.12\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 166.36 | loss  4.81 | ppl   123.28\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 163.91 | loss  4.88 | ppl   131.40\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 163.88 | loss  4.88 | ppl   131.36\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 163.51 | loss  4.88 | ppl   131.63\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 163.60 | loss  4.91 | ppl   136.05\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 163.07 | loss  4.98 | ppl   146.13\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 164.54 | loss  4.87 | ppl   130.21\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 163.78 | loss  4.90 | ppl   133.74\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 164.21 | loss  4.80 | ppl   121.85\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 162.69 | loss  4.84 | ppl   126.95\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 163.68 | loss  4.87 | ppl   130.05\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 169.65 | loss  4.80 | ppl   121.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 512.71s | valid loss  5.11 | valid ppl   165.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.02 | test ppl   151.87\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  400, nlayers: 1\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 194.56 | loss  7.51 | ppl  1830.66\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 194.55 | loss  6.60 | ppl   732.47\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 195.61 | loss  6.22 | ppl   502.99\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 195.78 | loss  6.06 | ppl   426.82\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 195.91 | loss  5.93 | ppl   377.85\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 195.83 | loss  5.86 | ppl   350.89\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 196.10 | loss  5.77 | ppl   319.10\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 196.43 | loss  5.78 | ppl   324.24\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 196.75 | loss  5.62 | ppl   276.29\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 196.62 | loss  5.60 | ppl   269.53\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 195.77 | loss  5.48 | ppl   240.66\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 197.19 | loss  5.50 | ppl   244.60\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 217.48 | loss  5.49 | ppl   242.99\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 200.20 | loss  5.38 | ppl   216.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 619.91s | valid loss  5.41 | valid ppl   222.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 203.98 | loss  5.38 | ppl   216.01\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 199.21 | loss  5.35 | ppl   210.33\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 200.30 | loss  5.16 | ppl   173.30\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 197.99 | loss  5.17 | ppl   175.32\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 207.66 | loss  5.14 | ppl   170.20\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 207.87 | loss  5.12 | ppl   166.53\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 199.09 | loss  5.12 | ppl   167.27\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 200.79 | loss  5.18 | ppl   177.80\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 198.06 | loss  5.04 | ppl   153.97\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 197.36 | loss  5.05 | ppl   156.47\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 197.48 | loss  4.94 | ppl   139.46\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 196.96 | loss  4.97 | ppl   144.35\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 197.89 | loss  4.99 | ppl   146.87\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 197.26 | loss  4.90 | ppl   134.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 623.88s | valid loss  5.13 | valid ppl   169.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 197.95 | loss  4.95 | ppl   140.74\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 205.92 | loss  4.96 | ppl   142.67\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 196.39 | loss  4.75 | ppl   115.78\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 196.17 | loss  4.79 | ppl   120.10\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 198.46 | loss  4.78 | ppl   119.18\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 197.23 | loss  4.77 | ppl   118.09\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 197.38 | loss  4.80 | ppl   121.83\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 200.09 | loss  4.87 | ppl   129.92\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 197.84 | loss  4.73 | ppl   113.75\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 197.32 | loss  4.76 | ppl   116.99\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 197.97 | loss  4.64 | ppl   103.66\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 200.96 | loss  4.69 | ppl   108.36\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 195.92 | loss  4.70 | ppl   110.39\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 196.47 | loss  4.62 | ppl   101.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 617.45s | valid loss  5.04 | valid ppl   154.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 198.81 | loss  4.68 | ppl   107.83\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 196.72 | loss  4.70 | ppl   110.18\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 198.16 | loss  4.50 | ppl    89.87\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 198.40 | loss  4.54 | ppl    94.00\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 198.03 | loss  4.55 | ppl    94.37\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 196.10 | loss  4.54 | ppl    94.04\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 196.24 | loss  4.58 | ppl    97.93\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 197.39 | loss  4.65 | ppl   104.36\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 198.50 | loss  4.52 | ppl    91.90\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 197.49 | loss  4.55 | ppl    95.02\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 198.05 | loss  4.44 | ppl    84.52\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 197.34 | loss  4.48 | ppl    87.94\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 197.35 | loss  4.51 | ppl    90.72\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 197.38 | loss  4.43 | ppl    83.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 615.89s | valid loss  4.98 | valid ppl   145.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 198.66 | loss  4.48 | ppl    88.63\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 204.92 | loss  4.51 | ppl    90.65\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 197.23 | loss  4.32 | ppl    74.83\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 196.54 | loss  4.36 | ppl    78.34\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 197.97 | loss  4.37 | ppl    79.08\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 196.14 | loss  4.38 | ppl    79.68\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 197.29 | loss  4.42 | ppl    82.88\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 196.48 | loss  4.48 | ppl    88.55\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 196.63 | loss  4.37 | ppl    78.77\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 195.89 | loss  4.40 | ppl    81.44\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 196.14 | loss  4.28 | ppl    72.15\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 196.11 | loss  4.32 | ppl    74.99\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 195.61 | loss  4.36 | ppl    78.11\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 195.93 | loss  4.28 | ppl    72.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 614.67s | valid loss  4.96 | valid ppl   141.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.89 | test ppl   132.66\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  400, nlayers: 2\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 235.08 | loss  7.68 | ppl  2163.41\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 234.87 | loss  6.82 | ppl   914.30\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 235.95 | loss  6.43 | ppl   619.33\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 240.58 | loss  6.22 | ppl   501.39\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 240.50 | loss  6.06 | ppl   430.20\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 240.07 | loss  5.97 | ppl   392.85\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 240.46 | loss  5.86 | ppl   350.04\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 235.96 | loss  5.86 | ppl   349.92\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 235.51 | loss  5.69 | ppl   297.19\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 236.02 | loss  5.66 | ppl   286.55\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 234.85 | loss  5.54 | ppl   255.67\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 242.69 | loss  5.56 | ppl   258.61\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 240.18 | loss  5.54 | ppl   253.64\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 239.90 | loss  5.43 | ppl   227.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 744.42s | valid loss  5.51 | valid ppl   246.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 240.19 | loss  5.43 | ppl   229.23\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 239.89 | loss  5.42 | ppl   225.76\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 241.52 | loss  5.22 | ppl   185.67\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 239.87 | loss  5.23 | ppl   187.66\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 239.99 | loss  5.20 | ppl   182.03\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 240.60 | loss  5.18 | ppl   178.30\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 241.02 | loss  5.19 | ppl   178.82\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 241.60 | loss  5.24 | ppl   188.17\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 241.24 | loss  5.09 | ppl   162.26\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 240.24 | loss  5.10 | ppl   163.75\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 241.26 | loss  4.99 | ppl   146.73\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 238.68 | loss  5.03 | ppl   152.36\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 239.77 | loss  5.03 | ppl   152.87\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 241.60 | loss  4.95 | ppl   140.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 752.81s | valid loss  5.24 | valid ppl   187.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 237.98 | loss  5.01 | ppl   149.37\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 241.12 | loss  5.02 | ppl   150.95\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 245.52 | loss  4.82 | ppl   123.94\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 251.30 | loss  4.86 | ppl   129.13\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 245.77 | loss  4.85 | ppl   127.70\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 249.71 | loss  4.84 | ppl   126.15\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 238.68 | loss  4.87 | ppl   130.39\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 237.91 | loss  4.93 | ppl   138.69\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 250.68 | loss  4.79 | ppl   120.58\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 257.94 | loss  4.81 | ppl   123.12\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 240.63 | loss  4.71 | ppl   110.61\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 241.65 | loss  4.75 | ppl   115.10\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 239.76 | loss  4.76 | ppl   116.76\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 240.65 | loss  4.69 | ppl   108.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 762.47s | valid loss  5.07 | valid ppl   158.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 243.27 | loss  4.74 | ppl   114.69\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 246.39 | loss  4.77 | ppl   117.55\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 275.15 | loss  4.57 | ppl    96.85\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 260.65 | loss  4.62 | ppl   101.83\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 266.41 | loss  4.62 | ppl   101.66\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 256.51 | loss  4.62 | ppl   101.29\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 255.58 | loss  4.66 | ppl   105.85\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 256.45 | loss  4.72 | ppl   112.49\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 255.02 | loss  4.60 | ppl    99.56\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 258.12 | loss  4.62 | ppl   101.34\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 247.85 | loss  4.52 | ppl    91.39\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 242.97 | loss  4.56 | ppl    95.49\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 241.91 | loss  4.57 | ppl    96.85\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 244.08 | loss  4.51 | ppl    90.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 788.58s | valid loss  4.99 | valid ppl   146.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 241.00 | loss  4.56 | ppl    95.56\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 240.22 | loss  4.59 | ppl    98.46\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 240.91 | loss  4.40 | ppl    81.47\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 241.51 | loss  4.45 | ppl    85.92\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 242.02 | loss  4.46 | ppl    86.42\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 240.58 | loss  4.46 | ppl    86.54\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 240.67 | loss  4.51 | ppl    90.94\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 240.71 | loss  4.57 | ppl    96.75\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 247.20 | loss  4.46 | ppl    86.11\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 1014.74 | loss  4.48 | ppl    88.41\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 223.69 | loss  4.37 | ppl    78.95\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 231.27 | loss  4.42 | ppl    82.79\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 238.14 | loss  4.44 | ppl    84.49\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 240.51 | loss  4.37 | ppl    79.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 902.02s | valid loss  4.95 | valid ppl   140.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.87 | test ppl   130.87\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  400, nlayers: 3\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 275.89 | loss  7.79 | ppl  2411.44\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 279.00 | loss  7.17 | ppl  1297.41\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 280.05 | loss  6.74 | ppl   843.94\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 279.74 | loss  6.55 | ppl   700.96\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 279.89 | loss  6.38 | ppl   587.40\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 280.10 | loss  6.24 | ppl   514.62\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 280.13 | loss  6.10 | ppl   445.62\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 280.20 | loss  6.07 | ppl   434.41\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 280.78 | loss  5.92 | ppl   372.11\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 281.61 | loss  5.88 | ppl   356.39\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 280.14 | loss  5.75 | ppl   314.20\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 281.29 | loss  5.74 | ppl   310.23\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 280.38 | loss  5.70 | ppl   299.72\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 281.99 | loss  5.58 | ppl   263.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1432.05s | valid loss  5.60 | valid ppl   271.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 329.38 | loss  5.57 | ppl   261.27\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 262.03 | loss  5.55 | ppl   257.76\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 266.90 | loss  5.38 | ppl   216.69\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 273.08 | loss  5.39 | ppl   218.93\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 278.89 | loss  5.36 | ppl   212.04\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 281.56 | loss  5.33 | ppl   207.11\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 281.57 | loss  5.32 | ppl   204.42\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 282.66 | loss  5.38 | ppl   216.21\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 281.54 | loss  5.24 | ppl   187.79\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 280.82 | loss  5.24 | ppl   188.92\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 280.80 | loss  5.14 | ppl   170.60\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 284.88 | loss  5.16 | ppl   174.63\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 280.75 | loss  5.17 | ppl   175.80\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 277.56 | loss  5.07 | ppl   158.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 879.89s | valid loss  5.26 | valid ppl   192.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 280.98 | loss  5.12 | ppl   168.05\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 278.68 | loss  5.14 | ppl   170.04\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 278.42 | loss  4.94 | ppl   139.84\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 280.15 | loss  4.98 | ppl   146.14\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 279.69 | loss  4.98 | ppl   144.84\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 285.42 | loss  4.97 | ppl   143.62\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 285.26 | loss  4.99 | ppl   146.74\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 279.56 | loss  5.06 | ppl   157.44\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 279.62 | loss  4.92 | ppl   136.96\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 274.69 | loss  4.94 | ppl   139.92\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 273.53 | loss  4.84 | ppl   126.83\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 278.11 | loss  4.87 | ppl   130.53\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 277.50 | loss  4.88 | ppl   132.27\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 276.99 | loss  4.81 | ppl   122.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 874.47s | valid loss  5.11 | valid ppl   166.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 275.08 | loss  4.86 | ppl   129.33\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 282.43 | loss  4.89 | ppl   132.86\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 282.84 | loss  4.69 | ppl   109.01\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 279.92 | loss  4.74 | ppl   114.98\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 279.35 | loss  4.74 | ppl   114.63\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 274.86 | loss  4.74 | ppl   114.89\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 275.16 | loss  4.78 | ppl   119.23\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 275.37 | loss  4.85 | ppl   128.04\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 273.68 | loss  4.72 | ppl   111.89\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 275.22 | loss  4.74 | ppl   114.60\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 274.57 | loss  4.64 | ppl   103.58\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 275.10 | loss  4.68 | ppl   107.49\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 275.36 | loss  4.70 | ppl   109.64\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 275.20 | loss  4.62 | ppl   101.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 866.20s | valid loss  5.02 | valid ppl   151.69\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 277.73 | loss  4.68 | ppl   107.96\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 275.79 | loss  4.71 | ppl   111.11\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 273.96 | loss  4.51 | ppl    91.19\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 274.72 | loss  4.57 | ppl    96.74\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 278.98 | loss  4.58 | ppl    97.59\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 283.21 | loss  4.58 | ppl    97.63\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 280.80 | loss  4.62 | ppl   101.04\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 281.28 | loss  4.70 | ppl   109.70\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 287.77 | loss  4.57 | ppl    96.98\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 276.31 | loss  4.59 | ppl    98.88\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 276.96 | loss  4.49 | ppl    88.92\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 275.40 | loss  4.53 | ppl    92.40\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 285.79 | loss  4.55 | ppl    94.54\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 279.12 | loss  4.48 | ppl    88.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 873.15s | valid loss  4.98 | valid ppl   145.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.92 | test ppl   137.64\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  600, nlayers: 1\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 282.25 | loss  7.55 | ppl  1893.98\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 285.69 | loss  6.56 | ppl   702.76\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 286.84 | loss  6.19 | ppl   487.28\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 287.28 | loss  6.04 | ppl   418.25\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 288.74 | loss  5.91 | ppl   367.48\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 289.26 | loss  5.83 | ppl   341.68\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 289.25 | loss  5.74 | ppl   309.92\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 288.14 | loss  5.75 | ppl   312.97\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 288.10 | loss  5.58 | ppl   265.56\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 288.43 | loss  5.54 | ppl   254.47\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 289.73 | loss  5.43 | ppl   228.47\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 289.35 | loss  5.45 | ppl   232.22\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 291.63 | loss  5.44 | ppl   229.85\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 289.56 | loss  5.32 | ppl   203.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 898.15s | valid loss  5.41 | valid ppl   223.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 290.76 | loss  5.31 | ppl   203.06\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 289.98 | loss  5.29 | ppl   198.71\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 289.34 | loss  5.08 | ppl   161.45\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 288.24 | loss  5.09 | ppl   161.95\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 288.72 | loss  5.06 | ppl   157.40\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 289.68 | loss  5.03 | ppl   153.67\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 289.81 | loss  5.04 | ppl   154.18\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 290.32 | loss  5.10 | ppl   164.17\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 285.35 | loss  4.94 | ppl   140.21\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 284.33 | loss  4.96 | ppl   142.01\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 284.02 | loss  4.85 | ppl   127.39\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 289.61 | loss  4.88 | ppl   131.07\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 289.26 | loss  4.89 | ppl   132.71\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 289.77 | loss  4.79 | ppl   120.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 899.32s | valid loss  5.10 | valid ppl   163.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 290.88 | loss  4.84 | ppl   126.58\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 297.80 | loss  4.86 | ppl   128.48\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 290.67 | loss  4.64 | ppl   103.46\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 290.41 | loss  4.67 | ppl   106.41\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 290.77 | loss  4.66 | ppl   105.35\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 292.20 | loss  4.65 | ppl   104.84\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 285.68 | loss  4.68 | ppl   108.10\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 285.28 | loss  4.75 | ppl   115.57\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 289.76 | loss  4.61 | ppl   100.10\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 290.40 | loss  4.63 | ppl   102.65\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 295.74 | loss  4.51 | ppl    91.29\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 291.49 | loss  4.55 | ppl    94.55\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 291.05 | loss  4.57 | ppl    96.46\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 291.34 | loss  4.49 | ppl    88.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 906.82s | valid loss  4.99 | valid ppl   147.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 291.90 | loss  4.54 | ppl    93.26\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 292.11 | loss  4.56 | ppl    95.15\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 291.04 | loss  4.35 | ppl    77.77\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 295.78 | loss  4.39 | ppl    80.33\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 292.30 | loss  4.39 | ppl    80.71\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 290.90 | loss  4.39 | ppl    80.77\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 290.49 | loss  4.43 | ppl    84.14\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 292.05 | loss  4.50 | ppl    90.21\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 289.74 | loss  4.36 | ppl    78.54\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 290.02 | loss  4.39 | ppl    81.01\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 290.62 | loss  4.27 | ppl    71.65\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 290.38 | loss  4.31 | ppl    74.67\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 290.14 | loss  4.34 | ppl    76.90\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 290.09 | loss  4.27 | ppl    71.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 907.88s | valid loss  4.97 | valid ppl   144.62\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 292.66 | loss  4.31 | ppl    74.81\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 293.34 | loss  4.33 | ppl    75.78\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 291.47 | loss  4.15 | ppl    63.14\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 290.22 | loss  4.18 | ppl    65.04\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 294.23 | loss  4.19 | ppl    65.77\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 290.89 | loss  4.20 | ppl    66.41\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 290.38 | loss  4.23 | ppl    69.02\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 290.31 | loss  4.31 | ppl    74.34\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 322.90 | loss  4.18 | ppl    65.24\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 322.00 | loss  4.21 | ppl    67.18\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 292.38 | loss  4.08 | ppl    59.39\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 294.36 | loss  4.13 | ppl    62.18\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 294.91 | loss  4.16 | ppl    64.34\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 290.09 | loss  4.09 | ppl    59.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 925.30s | valid loss  4.95 | valid ppl   141.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.88 | test ppl   132.05\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  600, nlayers: 2\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 400.33 | loss  7.73 | ppl  2279.98\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 388.45 | loss  6.79 | ppl   892.69\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 386.22 | loss  6.36 | ppl   579.54\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 383.97 | loss  6.16 | ppl   473.35\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 382.56 | loss  6.01 | ppl   406.78\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 400.85 | loss  5.93 | ppl   374.97\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 448.61 | loss  5.81 | ppl   334.10\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 390.32 | loss  5.81 | ppl   335.26\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 405.77 | loss  5.64 | ppl   280.62\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 427.13 | loss  5.60 | ppl   270.42\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 408.78 | loss  5.48 | ppl   240.97\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 425.69 | loss  5.49 | ppl   243.23\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 395.06 | loss  5.48 | ppl   239.47\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 391.55 | loss  5.36 | ppl   211.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1252.40s | valid loss  5.47 | valid ppl   236.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 383.46 | loss  5.36 | ppl   213.54\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 379.92 | loss  5.35 | ppl   209.74\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 385.48 | loss  5.14 | ppl   170.79\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 387.16 | loss  5.15 | ppl   173.18\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 385.77 | loss  5.12 | ppl   167.28\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 386.61 | loss  5.09 | ppl   162.86\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 385.13 | loss  5.10 | ppl   163.60\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 381.38 | loss  5.16 | ppl   173.76\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 378.74 | loss  5.00 | ppl   148.04\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 379.23 | loss  5.01 | ppl   150.15\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 1065.15 | loss  4.89 | ppl   133.40\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 370.84 | loss  4.93 | ppl   139.01\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 427.15 | loss  4.93 | ppl   138.57\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 411.14 | loss  4.84 | ppl   125.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1353.24s | valid loss  5.15 | valid ppl   171.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 390.27 | loss  4.89 | ppl   132.98\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 387.21 | loss  4.91 | ppl   134.99\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 384.46 | loss  4.69 | ppl   108.84\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 393.68 | loss  4.74 | ppl   114.52\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 385.33 | loss  4.72 | ppl   112.18\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 394.12 | loss  4.71 | ppl   111.17\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 388.51 | loss  4.74 | ppl   114.88\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 389.93 | loss  4.81 | ppl   122.27\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 389.45 | loss  4.67 | ppl   106.22\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 393.37 | loss  4.69 | ppl   108.40\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 389.06 | loss  4.57 | ppl    96.15\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 388.33 | loss  4.61 | ppl   100.94\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 387.78 | loss  4.62 | ppl   101.74\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 397.99 | loss  4.54 | ppl    93.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1222.45s | valid loss  5.02 | valid ppl   151.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 395.72 | loss  4.60 | ppl    99.00\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 387.79 | loss  4.62 | ppl   101.56\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 393.61 | loss  4.41 | ppl    82.39\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 396.37 | loss  4.46 | ppl    86.62\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 398.63 | loss  4.45 | ppl    86.04\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 401.76 | loss  4.45 | ppl    85.86\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 388.77 | loss  4.49 | ppl    89.43\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 386.80 | loss  4.56 | ppl    95.83\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 394.50 | loss  4.43 | ppl    83.81\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 402.79 | loss  4.45 | ppl    85.89\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 409.25 | loss  4.34 | ppl    76.41\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 395.89 | loss  4.39 | ppl    80.77\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 393.94 | loss  4.39 | ppl    81.04\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 404.52 | loss  4.33 | ppl    75.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 1238.89s | valid loss  4.96 | valid ppl   142.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 392.30 | loss  4.38 | ppl    80.17\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 423.28 | loss  4.41 | ppl    81.92\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 409.79 | loss  4.20 | ppl    66.71\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 401.69 | loss  4.25 | ppl    69.94\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 391.81 | loss  4.26 | ppl    70.88\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 388.81 | loss  4.26 | ppl    70.68\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 387.59 | loss  4.31 | ppl    74.09\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 386.41 | loss  4.37 | ppl    79.40\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 385.39 | loss  4.25 | ppl    70.09\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 383.01 | loss  4.28 | ppl    72.02\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 385.09 | loss  4.16 | ppl    63.97\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 385.40 | loss  4.21 | ppl    67.28\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 385.38 | loss  4.23 | ppl    68.53\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 384.16 | loss  4.16 | ppl    64.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 1225.15s | valid loss  4.92 | valid ppl   137.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.88 | test ppl   131.37\n",
      "=========================================================================================\n",
      "Running experiment for: emsize_nhid:  600, nlayers: 3\n",
      "| epoch   1 |   200/ 2983 batches | lr 20.00 | ms/batch 474.27 | loss  7.86 | ppl  2578.87\n",
      "| epoch   1 |   400/ 2983 batches | lr 20.00 | ms/batch 478.69 | loss  7.09 | ppl  1205.20\n",
      "| epoch   1 |   600/ 2983 batches | lr 20.00 | ms/batch 476.08 | loss  6.71 | ppl   824.08\n",
      "| epoch   1 |   800/ 2983 batches | lr 20.00 | ms/batch 478.51 | loss  6.53 | ppl   684.29\n",
      "| epoch   1 |  1000/ 2983 batches | lr 20.00 | ms/batch 478.85 | loss  6.33 | ppl   561.10\n",
      "| epoch   1 |  1200/ 2983 batches | lr 20.00 | ms/batch 479.07 | loss  6.19 | ppl   490.15\n",
      "| epoch   1 |  1400/ 2983 batches | lr 20.00 | ms/batch 477.99 | loss  6.04 | ppl   419.89\n",
      "| epoch   1 |  1600/ 2983 batches | lr 20.00 | ms/batch 667.69 | loss  6.01 | ppl   409.36\n",
      "| epoch   1 |  1800/ 2983 batches | lr 20.00 | ms/batch 479.19 | loss  5.84 | ppl   343.10\n",
      "| epoch   1 |  2000/ 2983 batches | lr 20.00 | ms/batch 482.02 | loss  5.78 | ppl   325.26\n",
      "| epoch   1 |  2200/ 2983 batches | lr 20.00 | ms/batch 491.10 | loss  5.66 | ppl   285.86\n",
      "| epoch   1 |  2400/ 2983 batches | lr 20.00 | ms/batch 474.77 | loss  5.64 | ppl   282.65\n",
      "| epoch   1 |  2600/ 2983 batches | lr 20.00 | ms/batch 475.90 | loss  5.61 | ppl   272.29\n",
      "| epoch   1 |  2800/ 2983 batches | lr 20.00 | ms/batch 482.50 | loss  5.49 | ppl   242.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1541.45s | valid loss  5.57 | valid ppl   261.57\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2983 batches | lr 20.00 | ms/batch 488.40 | loss  5.49 | ppl   242.75\n",
      "| epoch   2 |   400/ 2983 batches | lr 20.00 | ms/batch 488.69 | loss  5.47 | ppl   238.28\n",
      "| epoch   2 |   600/ 2983 batches | lr 20.00 | ms/batch 486.67 | loss  5.29 | ppl   197.94\n",
      "| epoch   2 |   800/ 2983 batches | lr 20.00 | ms/batch 489.57 | loss  5.31 | ppl   201.73\n",
      "| epoch   2 |  1000/ 2983 batches | lr 20.00 | ms/batch 484.42 | loss  5.27 | ppl   193.86\n",
      "| epoch   2 |  1200/ 2983 batches | lr 20.00 | ms/batch 483.98 | loss  5.24 | ppl   187.94\n",
      "| epoch   2 |  1400/ 2983 batches | lr 20.00 | ms/batch 482.02 | loss  5.23 | ppl   186.42\n",
      "| epoch   2 |  1600/ 2983 batches | lr 20.00 | ms/batch 487.11 | loss  5.27 | ppl   195.28\n",
      "| epoch   2 |  1800/ 2983 batches | lr 20.00 | ms/batch 485.75 | loss  5.13 | ppl   169.49\n",
      "| epoch   2 |  2000/ 2983 batches | lr 20.00 | ms/batch 483.64 | loss  5.13 | ppl   169.43\n",
      "| epoch   2 |  2200/ 2983 batches | lr 20.00 | ms/batch 482.15 | loss  5.02 | ppl   151.78\n",
      "| epoch   2 |  2400/ 2983 batches | lr 20.00 | ms/batch 482.87 | loss  5.04 | ppl   154.89\n",
      "| epoch   2 |  2600/ 2983 batches | lr 20.00 | ms/batch 484.49 | loss  5.04 | ppl   154.27\n",
      "| epoch   2 |  2800/ 2983 batches | lr 20.00 | ms/batch 483.80 | loss  4.95 | ppl   141.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1520.27s | valid loss  5.22 | valid ppl   184.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2983 batches | lr 20.00 | ms/batch 485.96 | loss  5.01 | ppl   149.33\n",
      "| epoch   3 |   400/ 2983 batches | lr 20.00 | ms/batch 490.86 | loss  5.02 | ppl   151.19\n",
      "| epoch   3 |   600/ 2983 batches | lr 20.00 | ms/batch 485.39 | loss  4.82 | ppl   124.46\n",
      "| epoch   3 |   800/ 2983 batches | lr 20.00 | ms/batch 486.00 | loss  4.87 | ppl   129.77\n",
      "| epoch   3 |  1000/ 2983 batches | lr 20.00 | ms/batch 481.36 | loss  4.85 | ppl   127.26\n",
      "| epoch   3 |  1200/ 2983 batches | lr 20.00 | ms/batch 472.51 | loss  4.83 | ppl   125.06\n",
      "| epoch   3 |  1400/ 2983 batches | lr 20.00 | ms/batch 481.12 | loss  4.86 | ppl   128.77\n",
      "| epoch   3 |  1600/ 2983 batches | lr 20.00 | ms/batch 476.66 | loss  4.92 | ppl   137.56\n",
      "| epoch   3 |  1800/ 2983 batches | lr 20.00 | ms/batch 480.79 | loss  4.79 | ppl   119.97\n",
      "| epoch   3 |  2000/ 2983 batches | lr 20.00 | ms/batch 478.63 | loss  4.81 | ppl   122.37\n",
      "| epoch   3 |  2200/ 2983 batches | lr 20.00 | ms/batch 480.39 | loss  4.69 | ppl   108.92\n",
      "| epoch   3 |  2400/ 2983 batches | lr 20.00 | ms/batch 481.66 | loss  4.73 | ppl   112.97\n",
      "| epoch   3 |  2600/ 2983 batches | lr 20.00 | ms/batch 486.58 | loss  4.73 | ppl   113.59\n",
      "| epoch   3 |  2800/ 2983 batches | lr 20.00 | ms/batch 5469.73 | loss  4.66 | ppl   105.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 4406.03s | valid loss  5.09 | valid ppl   161.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2983 batches | lr 20.00 | ms/batch 5418.96 | loss  4.71 | ppl   110.96\n",
      "| epoch   4 |   400/ 2983 batches | lr 20.00 | ms/batch 461.90 | loss  4.74 | ppl   114.65\n",
      "| epoch   4 |   600/ 2983 batches | lr 20.00 | ms/batch 463.99 | loss  4.54 | ppl    93.30\n",
      "| epoch   4 |   800/ 2983 batches | lr 20.00 | ms/batch 470.48 | loss  4.59 | ppl    98.14\n",
      "| epoch   4 |  1000/ 2983 batches | lr 20.00 | ms/batch 471.92 | loss  4.58 | ppl    97.98\n",
      "| epoch   4 |  1200/ 2983 batches | lr 20.00 | ms/batch 473.83 | loss  4.57 | ppl    96.64\n",
      "| epoch   4 |  1400/ 2983 batches | lr 20.00 | ms/batch 474.43 | loss  4.62 | ppl   101.25\n",
      "| epoch   4 |  1600/ 2983 batches | lr 20.00 | ms/batch 566.64 | loss  4.69 | ppl   108.64\n",
      "| epoch   4 |  1800/ 2983 batches | lr 20.00 | ms/batch 479.14 | loss  4.56 | ppl    95.80\n",
      "| epoch   4 |  2000/ 2983 batches | lr 20.00 | ms/batch 483.53 | loss  4.58 | ppl    97.75\n",
      "| epoch   4 |  2200/ 2983 batches | lr 20.00 | ms/batch 485.92 | loss  4.46 | ppl    86.60\n",
      "| epoch   4 |  2400/ 2983 batches | lr 20.00 | ms/batch 482.65 | loss  4.50 | ppl    90.25\n",
      "| epoch   4 |  2600/ 2983 batches | lr 20.00 | ms/batch 482.35 | loss  4.52 | ppl    91.67\n",
      "| epoch   4 |  2800/ 2983 batches | lr 20.00 | ms/batch 484.39 | loss  4.45 | ppl    85.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 2622.66s | valid loss  5.03 | valid ppl   153.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2983 batches | lr 20.00 | ms/batch 476.59 | loss  4.50 | ppl    90.37\n",
      "| epoch   5 |   400/ 2983 batches | lr 20.00 | ms/batch 478.56 | loss  4.53 | ppl    93.02\n",
      "| epoch   5 |   600/ 2983 batches | lr 20.00 | ms/batch 476.91 | loss  4.34 | ppl    76.70\n",
      "| epoch   5 |   800/ 2983 batches | lr 20.00 | ms/batch 476.09 | loss  4.38 | ppl    80.18\n",
      "| epoch   5 |  1000/ 2983 batches | lr 20.00 | ms/batch 485.54 | loss  4.39 | ppl    80.69\n",
      "| epoch   5 |  1200/ 2983 batches | lr 20.00 | ms/batch 2050.81 | loss  4.39 | ppl    80.71\n",
      "| epoch   5 |  1400/ 2983 batches | lr 20.00 | ms/batch 464.89 | loss  4.44 | ppl    84.47\n",
      "| epoch   5 |  1600/ 2983 batches | lr 20.00 | ms/batch 478.11 | loss  4.51 | ppl    90.92\n",
      "| epoch   5 |  1800/ 2983 batches | lr 20.00 | ms/batch 485.18 | loss  4.38 | ppl    80.09\n",
      "| epoch   5 |  2000/ 2983 batches | lr 20.00 | ms/batch 483.24 | loss  4.42 | ppl    82.69\n",
      "| epoch   5 |  2200/ 2983 batches | lr 20.00 | ms/batch 485.25 | loss  4.29 | ppl    72.67\n",
      "| epoch   5 |  2400/ 2983 batches | lr 20.00 | ms/batch 486.36 | loss  4.33 | ppl    76.16\n",
      "| epoch   5 |  2600/ 2983 batches | lr 20.00 | ms/batch 496.31 | loss  4.35 | ppl    77.59\n",
      "| epoch   5 |  2800/ 2983 batches | lr 20.00 | ms/batch 482.68 | loss  4.29 | ppl    72.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 1821.92s | valid loss  4.97 | valid ppl   144.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.90 | test ppl   134.87\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "config = Config()\n",
    "config.epochs = 5 #shorten # of epoch for time efficiency\n",
    "for emsize_nhid in emsize_nhid_values:\n",
    "    for nlayers in nlayers_values:\n",
    "        config.emsize = emsize_nhid\n",
    "        config.nhid = emsize_nhid\n",
    "        config.nlayers = nlayers\n",
    "\n",
    "        if emsize_nhid == 200 and nlayers == 2:\n",
    "            continue  # Skip this iteration - already have results from part 1\n",
    "        print(f\"Running experiment for: emsize_nhid:  {emsize_nhid}, nlayers: {nlayers}\")\n",
    "        test_ppl = main(config)\n",
    "        results[(emsize_nhid, nlayers)] = test_ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert the default setting (nhid,nlayers) = (200,2) ppl from the result we got from part 1\n",
    "\n",
    "- Note that part 1 test ppl is from running 40 epochs on LSTM model. Due to time and resource constraints, we only run 5 epochs for this part. \n",
    "- We will use the valid ppl at the 5th epoch from results from part 1 for both models and insert that in our result arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(200, 1): 143.77090199371608, (200, 3): 151.87475195402064, (400, 1): 132.66005903254367, (400, 2): 130.86974352813272, (400, 3): 137.64010912026978, (600, 1): 132.04536211588263, (600, 2): 131.36822508468444, (600, 3): 134.87010264870946, (200, 2): 153.89}\n"
     ]
    }
   ],
   "source": [
    "results[(200,2)] = 153.89\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABQ8UlEQVR4nO3dd1xV9f8H8Ne57K2AMhTBvcAVmSMVcpADNUsqR4jaT0NTv6KWlYIT915pJm4borlHiitHCpJpigsMB1GKInvc8/uDuHkF9Z57z5Xr5fXscR6Pez7nc855nxvK2886giiKIoiIiIiMlKKsAyAiIiLSJyY7REREZNSY7BAREZFRY7JDRERERo3JDhERERk1JjtERERk1JjsEBERkVFjskNERERGjckOERERGTUmO6QzQRA02o4cOaLzvbKyshAREaHxtZKSktRiUCgUcHJyQpcuXXDq1KnnPoeDgwP8/Pywe/dutXpeXl7o1q2b5NgjIiIgCAL++ecfyefqw5EjRyAIAn788ccX1h0wYAC8vLw0uq4gCIiIiNAtOBkVf+9yKf6ZioqKku2aRKRfpmUdAL36nk4apkyZgpiYGBw+fFitvEGDBjrfKysrC5MmTQIA+Pn5aXzep59+ij59+qCwsBCXLl3CpEmT4O/vj1OnTqFp06aqeu+99x7CwsKgVCpx8+ZNTJ06FYGBgdi5cye6du2qc/yvqgkTJmDkyJFlHQYRkVaY7JDOWrRoobZfqVIlKBSKEuVlqVq1aqp4WrdujVq1aqF9+/ZYtmwZVq1aparn4uKiqteqVSu0bNkStWrVwoIFC8p1slOzZs2yDoE0VFhYiIKCAlhYWJR1KEQGg91Y9FLk5eVh6tSpqFevHiwsLFCpUiWEhITg77//Vqt3+PBh+Pn5wcnJCVZWVqhWrRreffddZGVlISkpCZUqVQIATJo0SdXdNGDAAMnxFCc0t27dem69mjVrolKlSi+sJ0VycjJ69eoFe3t7ODg4oF+/fmrfw6BBg+Do6IisrKwS57711lto2LDhc6/v5+cHb29vnD17Fm3atIG1tTVq1KiBGTNmQKlUlqifn5+PL7/8Eu7u7rC3t0eHDh2QkJCgVqe0bqz09HR8/PHHcHJygq2tLd5++21cvXpVo+8gJycHYWFhaNKkCRwcHODo6IiWLVvip59+KlFXEAQMHz4c69evR/369WFtbY3GjRtj165dJeru3r0bTZo0gYWFBapXr445c+ZoFA8g/Xt70vXr1xESEoLatWvD2toaVapUQWBgIH7//XdVnYyMDFSoUAFDhgwpcX5SUhJMTEwwe/ZsVVlKSgqGDBmCqlWrwtzcHNWrV8ekSZNQUFCgdp4gCJg1axamTp2K6tWrw8LCAjExMVAqlZg6dSrq1q0LKysrVKhQAY0aNcLChQs1/k6IjAWTHdI7pVKJHj16YMaMGejTpw92796NGTNm4ODBg/Dz80N2djaAor+4u3btCnNzc3z77bfYt28fZsyYARsbG+Tl5cHNzQ379u0DUJQQnDp1CqdOncKECRMkx3T9+nUAUCVPz5KWlob79++/sJ4U77zzDmrVqoUff/wRERER2L59OwICApCfnw8AGDlyJNLS0rBp0ya18/744w/ExMRg2LBhL7xHSkoK+vbti379+mHHjh3o3Lkzxo8fjw0bNpSo+8UXX+DWrVv45ptvsHLlSly7dg2BgYEoLCx85vVFUUTPnj2xfv16hIWFYdu2bWjRogU6d+6s0XeQm5uLBw8eYMyYMdi+fTs2b96MN998E7169cK6detK1N+9ezeWLFmCyZMnY+vWrXB0dMQ777yDmzdvquocOnQIPXr0gJ2dHbZs2YLZs2fj+++/x5o1azSKCZD2vT3p7t27cHJywowZM7Bv3z4sXboUpqameOONN1SJo62tLQYOHIiNGzfi0aNHaucvW7YM5ubmGDhwoCqO5s2bY//+/Zg4cSL27t2LQYMGITIyEh9//HGJ+y9atAiHDx/GnDlzsHfvXtSrVw+zZs1CREQEPvzwQ+zevRvfffcdBg0ahIcPH2r8fRAZDZFIZsHBwaKNjY1qf/PmzSIAcevWrWr1zp49KwIQly1bJoqiKP74448iADE+Pv6Z1/77779FAGJ4eLhGsSQmJooAxJkzZ4r5+fliTk6OGBsbK77++usiAHH37t2qugDE0NBQMT8/X8zLyxMvX74sdu7cWQQgLl26VFXP09NT7Nq1q0b3f1J4eLgIQPzf//6nVr5x40YRgLhhwwZVWbt27cQmTZqo1fvkk09Ee3t78fHjx8+9T7t27UQA4pkzZ9TKGzRoIAYEBKj2Y2JiRABily5d1Op9//33IgDx1KlTqrLg4GDR09NTtb93714RgLhw4UK1c6dNmybp/0+xgoICMT8/Xxw0aJDYtGlTtWMARBcXFzE9PV1VlpKSIioUCjEyMlJV9sYbb4ju7u5idna2qiw9PV10dHQUNfmrTtPvrfhnas2aNc99nry8PLF27dpq/79v3LghKhQKcf78+aqy7Oxs0cnJSQwJCVGVDRkyRLS1tRVv3bqldt05c+aIAMRLly6pxVKzZk0xLy9PrW63bt1K/AwRlVds2SG927VrFypUqIDAwEAUFBSotiZNmsDV1VU1s6pJkyYwNzfH//3f/2Ht2rVq/2rX1WeffQYzMzNYWlritddew59//omvv/4aXbp0Uau3bNkymJmZwdzcHPXr18fJkycxefJkhIaGyhZL37591faDgoJgamqKmJgYVdnIkSMRHx+PX375BUBRl9H69esRHBwMW1vbF97D1dUVzZs3Vytr1KhRqd1x3bt3L1EPeH4XX3GsTz9Lnz59XhhbsR9++AGtW7eGra0tTE1NYWZmhtWrV+Py5csl6vr7+8POzk617+LigsqVK6tizMzMxNmzZ9GrVy9YWlqq6tnZ2SEwMFDjmKR8b08qKCjA9OnT0aBBA5ibm8PU1BTm5ua4du2a2vPUqFED3bp1w7JlyyCKIgBg06ZNuH//PoYPH66qt2vXLvj7+8Pd3V3tz0xxy9nRo0fV7t+9e3eYmZmplTVv3hy//fYbQkNDsX//fqSnp2v8PRAZGyY7pHd//fUXHj58CHNzc5iZmaltKSkpqqnYNWvWxM8//4zKlStj2LBhqFmzJmrWrCnLGIORI0fi7NmziI2NxY0bN3Dv3j383//9X4l6QUFBOHv2LM6dO4eEhATcv39fq26y53F1dVXbNzU1hZOTE+7fv68q69GjB7y8vLB06VIAQFRUFDIzMzXqwgIAJyenEmUWFhaqLsPn1S0e2Fpa3WL3799Xxf2kp5/tWaKjoxEUFIQqVapgw4YNOHXqFM6ePYuBAwciJydH8vOkpaVBqVSWen9NY9LkPs8yevRoTJgwAT179sTOnTtx5swZnD17Fo0bNy5x7siRI3Ht2jUcPHgQALB06VK0bNkSzZo1U9X566+/sHPnzhJ/XorHaz29fIGbm1uJmMaPH485c+bg9OnT6Ny5M5ycnNC+fXucO3dOsy+DyIhwNhbpnbOzM5ycnFTjbZ725L/Y27RpgzZt2qCwsBDnzp3D4sWLMWrUKLi4uOCDDz7QOoaqVavC19f3hfUqVaqkUT1dpKSkoEqVKqr9goIC3L9/X+0XrUKhwLBhw/DFF19g7ty5WLZsGdq3b4+6devqNTZNOTk5lRp3SkqKRudv2LAB1atXx3fffae2Bk5ubq5W8VSsWBGCIJR6f01j0sWGDRvw0UcfYfr06Wrl//zzDypUqKBW9tZbb8Hb2xtLliyBra0t4uLiSowJcnZ2RqNGjTBt2rRS7+fu7q62X9o6Qqamphg9ejRGjx6Nhw8f4ueff8YXX3yBgIAAJCcnw9raWosnJXo1sWWH9K5bt264f/8+CgsL4evrW2Ir7Re4iYkJ3njjDVXLRlxcHADNWh0M3caNG9X2v//+exQUFJRYN2jw4MEwNzdH3759kZCQoNbNUdb8/f0BlHyWpwdVP4sgCDA3N1f7JZ2SklLqbCxN2NjYoHnz5oiOjlZrGXr8+DF27typ1TWlEAShxFTv3bt3486dO6XWHzFiBHbv3o3x48fDxcUFvXv3VjverVs3XLx4ETVr1iz1z8zTyc6LVKhQAe+99x6GDRuGBw8eICkpSdL5RK86tuyQ3n3wwQfYuHEjunTpgpEjR6J58+YwMzPD7du3ERMTgx49euCdd97BihUrcPjwYXTt2hXVqlVDTk4Ovv32WwBAhw4dABS1Anl6euKnn35C+/bt4ejoCGdnZ41X95VLSkpKqSsPe3l5vbBlKDo6GqampujYsSMuXbqECRMmoHHjxggKClKrV6FCBXz00UdYvnw5PD09JY090bdOnTqhbdu2GDduHDIzM+Hr64tffvkF69ev1+j8bt26ITo6GqGhoXjvvfeQnJyMKVOmwM3NDdeuXdMqpilTpuDtt99Gx44dERYWhsLCQsycORM2NjZ48OCBVtfUVLdu3RAVFYV69eqhUaNGiI2NxezZs1G1atVS6/fr1w/jx4/HsWPH8NVXX8Hc3Fzt+OTJk3Hw4EG0atUKI0aMQN26dZGTk4OkpCTs2bMHK1aseOa1iwUGBsLb2xu+vr6q5RMWLFgAT09P1K5dW7ZnJ3oVMNkhvTMxMcGOHTuwcOFCrF+/HpGRkTA1NUXVqlXRrl07+Pj4ACgaoHzgwAGEh4cjJSUFtra28Pb2xo4dO9CpUyfV9VavXo2xY8eie/fuyM3NRXBw8Etfuj82NrbEv8YBaBRLdHQ0IiIisHz5cgiCgMDAQCxYsKDELzwAeP/997F8+XJ88sknUCgMpyFWoVBgx44dGD16NGbNmoW8vDy0bt0ae/bsQb169V54fkhICFJTU7FixQp8++23qFGjBj7//HPcvn1btUK2VB07dsT27dvx1Vdf4f3334erqytCQ0ORnZ2t9TU1tXDhQpiZmSEyMhIZGRlo1qwZoqOj8dVXX5Va38rKCoGBgdiwYQOGDh1a4ribmxvOnTuHKVOmYPbs2bh9+zbs7OxQvXp1vP3226hYseILY/L398fWrVvxzTffID09Ha6urujYsSMmTJhQYjAzkbETxOIpAURkcMLCwrB8+XIkJyeXOniWXk15eXnw8vLCm2++ie+//76swyEyemzZITJAp0+fxtWrV7Fs2TIMGTKEiY6R+Pvvv5GQkIA1a9bgr7/+wueff17WIRGVC0x2iAxQy5YtYW1tjW7dumHq1KllHQ7JZPfu3QgJCYGbmxuWLVumNt2ciPSH3VhERERk1AxnxCMRERGRHjDZISIiIqPGZIeIiIiMGgcoA1Aqlbh79y7s7OxKXXadiIiomCiKePz4Mdzd3fW6/lVOTg7y8vJ0vo65ubnaC3LLIyY7AO7evQsPD4+yDoOIiF4hycnJL1zJWls5OTlwsKqIPJR8Ma5Urq6uSExMLNcJD5Md/PciyuNnnGFry549ApxN+EeD/hP16MWrQlP5kZNRgGntj6m9xFhueXl5yEMO3kQXmEL7Fa8LkI8TKXuQl5fHZKe8K+66srVVwM6OyQ4B9ib8OaD/WBbyr0oq6WUMezAVLGAq6PB6D1EBcIEZJjtERESGSlAIOiVVgigAhTIG9IpiskNERGSoBEXRpjW2UgP8FoiIiMjIsWWHiIjIQMnSjUVMdoiIiAyWIOjYjcVkB2A3FhERERk5tuwQEREZKoVQ1LqjLXZjAWCyQ0REZLgEHZMddmMBYDcWERERGTm27BARERkoQaGAoMMAZUFkmwbAZIeIiMhwsRtLFkz5iIiIyKixZYeIiMhQKXR8XQS7sQAw2SEiIjJc7MaSBZMdIiIiQyUIRWvtaEvJZAfgmB0iIiIycmzZISIiMlCCoOPUc53eq2U8mOwQEREZKoWO3VgcswOA3VhERERk5NiyQ0REZKh0nY2l00wu48Fkh4iIyFAx2ZEFu7GIiIjIqLFlh4iIyFApFEWb9heQLZRXGZMdIiIiQ8VuLFkw5SMiIiKjxpYdIiIiQyVAx5Yd2SJ5pTHZISIiMlTsxpIFkx0iIiJDxRWUZcExO0RERGTU2LJDRERkqNiNJQsmO0RERIZKUBRtupxP7MYiIiIi48aWHSIiIkPFAcqyYLJDRERkqDhmRxbsxiIiIiKjxpYdIiIig6Vjyw67sQAw2SEiIjJc7MaSBbuxiIiIyKixZYeIiMhQ6TobS2TLDsBkh4iIyHCxG0sWTHaIiIgMlCgIEHVIWHQ515hwzA4REREZNbbsEBERGSoFdGuWEOUK5NXGZIeIiMhQccyOLNiNRUREREaNLTtERESGii07smCyQ0REZKiY7MiC3VhERERk1NiyQ0REZKC4zo48mOwQEREZKk49lwW7sYiIiMiosWWHiIjIUHGAsiyY7BARERkqATomO7JF8kpjskNERGSgOEBZHmU+ZufOnTvo168fnJycYG1tjSZNmiA2NlZ1XBRFREREwN3dHVZWVvDz88OlS5fUrpGbm4tPP/0Uzs7OsLGxQffu3XH79u2X/ShERERkgMo02UlLS0Pr1q1hZmaGvXv34o8//sDcuXNRoUIFVZ1Zs2Zh3rx5WLJkCc6ePQtXV1d07NgRjx8/VtUZNWoUtm3bhi1btuDEiRPIyMhAt27dUFhYWAZPRUREJBMB/83I0maT2LBz7NgxBAYGwt3dHYIgYPv27WrHBwwYAEEQ1LYWLVqo1THEBogyTXZmzpwJDw8PrFmzBs2bN4eXlxfat2+PmjVrAihq1VmwYAG+/PJL9OrVC97e3li7di2ysrKwadMmAMCjR4+wevVqzJ07Fx06dEDTpk2xYcMG/P777/j555/L8vGIiIh0UzxAWZdNgszMTDRu3BhLlix5Zp23334b9+7dU2179uxRO26IDRBlmuzs2LEDvr6+6N27NypXroymTZti1apVquOJiYlISUlBp06dVGUWFhZo164dTp48CQCIjY1Ffn6+Wh13d3d4e3ur6hAREdGLde7cGVOnTkWvXr2eWcfCwgKurq6qzdHRUXXMUBsgyjTZuXnzJpYvX47atWtj//79GDp0KEaMGIF169YBAFJSUgAALi4uaue5uLiojqWkpMDc3BwVK1Z8Zp2n5ebmIj09XW0jIiIyNMUDlHXZ5HbkyBFUrlwZderUwccff4zU1FTVMUNtgCjT2VhKpRK+vr6YPn06AKBp06a4dOkSli9fjo8++khVT3jqf5YoiiXKnva8OpGRkZg0aZKO0RMREemZAN2mj/977tP/qLewsICFhYXky3Xu3Bm9e/eGp6cnEhMTMWHCBLz11luIjY2FhYWFVg0QL0OZtuy4ubmhQYMGamX169fHn3/+CQBwdXUFgBJfUGpqqqq1x9XVFXl5eUhLS3tmnaeNHz8ejx49Um3JycmyPA8REZEh8vDwgIODg2qLjIzU6jrvv/8+unbtCm9vbwQGBmLv3r24evUqdu/e/dzzNGmk0KcyTXZat26NhIQEtbKrV6/C09MTAFC9enW4urri4MGDquN5eXk4evQoWrVqBQB47bXXYGZmplbn3r17uHjxoqrO0ywsLGBvb6+2ERERGRyZBignJyer/SN//PjxsoTn5uYGT09PXLt2DYB2DRAvQ5kmO//73/9w+vRpTJ8+HdevX8emTZuwcuVKDBs2DEBR99WoUaMwffp0bNu2DRcvXsSAAQNgbW2NPn36AAAcHBwwaNAghIWF4dChQzh//jz69esHHx8fdOjQoSwfj4iISCeiQvcNQIl/4GvThVWa+/fvIzk5GW5ubgC0a4B4GSSN2UlISMDmzZtx/PhxJCUlISsrC5UqVULTpk0REBCAd999V9IX+Prrr2Pbtm0YP348Jk+ejOrVq2PBggXo27evqs64ceOQnZ2N0NBQpKWl4Y033sCBAwdgZ2enqjN//nyYmpoiKCgI2dnZaN++PaKiomBiYiLl8YiIiMq1jIwMXL9+XbWfmJiI+Ph4ODo6wtHREREREXj33Xfh5uaGpKQkfPHFF3B2dsY777wDQL0BwsnJCY6OjhgzZkyZN0AIoii+8AXw58+fx7hx43D8+HG0atUKzZs3R5UqVWBlZYUHDx7g4sWLOH78ONLT0zFu3DiMGjVKtqzxZUhPT4eDgwPOX6oMO7syX1SaDEAlE75Jhf6z8mGDF1eiciMnowAT3jiMR48e6W0YRPHvpXatJsDU1FLr6xQU5ODoySkax3rkyBH4+/uXKA8ODsby5cvRs2dPnD9/Hg8fPoSbmxv8/f0xZcoUeHh4qOrm5ORg7Nix2LRpk6oBYtmyZWp1XjaN/kbv2bMnxo4di++++05tPv3TTp06hfnz52Pu3Ln44osvZAuSiIioPBKFok2X86Xw8/PD89pA9u/f/8JrWFpaYvHixVi8eLG0m+uRRsnOtWvXYG5u/sJ6LVu2RMuWLZGXl6dzYEREROWeFqsglzifNBug/KxEJycnR1J9IiIiopdN8gAVpVKJKVOmoEqVKrC1tcXNmzcBABMmTMDq1atlD5CIiKjcEmTYSHqyM3XqVERFRWHWrFlqLTg+Pj745ptvZA2OiIioPDPE10W8iiQnO+vWrcPKlSvRt29ftandjRo1wpUrV2QNjoiIiEhXkufX3rlzB7Vq1SpRrlQqkZ+fL0tQREREhKImCV1WROFqKgC0+BoaNmyI48ePlyj/4Ycf0LRpU1mCIiIiInZjyUVyy054eDj69++PO3fuQKlUIjo6GgkJCVi3bh127dqljxiJiIiItCa5ZScwMBDfffcd9uzZA0EQMHHiRFy+fBk7d+5Ex44d9REjERFR+cTZWLLQak38gIAABAQEyB0LERERPeFlr6BsrCS37AwcOBBr164tUZ6eno6BAwfKEhQRERGRXCQnO1FRUQgNDcWIESOgVCpV5dnZ2aUmQURERKSl4tdF6LKRdpPSdu/ejb179yIgIABpaWlyx0REREQARPzXlaXVVtYPYCC0SnYaNGiA06dPIz8/H6+//jouX74sd1xERETEAcqykJzsCP82iTk5OeHnn3+Gn58fWrRogR07dsgeHBEREZGuJM/GEsX/GsVMTU3xzTffoEGDBggNDZU1MCIionJPIRRtupxP0pOdmJgYODo6qpWNHj0ajRo1wi+//CJbYEREROUdp57LQ3Ky065du1LLO3TogA4dOugcEBEREZGcNEp2Ro8ejSlTpsDGxgajR49+bt158+bJEhgREVG5p+sgY7bsANAw2Tl//rzqjebnz59/Zj2B8/mJiIhkw24seWiU7MTExJT6mYiIiMjQabXOzpPS09Oxfft2XLlyRY54iIiIqBhXUJaF5GQnKCgIS5YsAVD0ighfX18EBQXBx8cHW7dulT1AIiKi8kqn1ZN17AIzJpKTnWPHjqFNmzYAgG3btkEURTx8+BCLFi3C1KlTZQ+QiIiISBeSk51Hjx6p1tnZt28f3n33XVhbW6Nr1664du2a7AESERGVW3xdhCwkJzseHh44deoUMjMzsW/fPnTq1AkAkJaWBktLS9kDJCIiKq/YjSUPyYsKjho1Cn379oWtrS08PT3h5+cHoKh7y8fHR+74iIiIyi9dBxlzgDIALZKd0NBQNG/eHMnJyejYsSMUiqLGoRo1anDMDhERERkcyckOAPj6+sLX11etrGvXrrIEREREREW4qKA8tHrr+Y8//oiYmBikpqZCqVSqHY+OjpYtOCIionKNr4uQheRkZ+TIkVi5ciX8/f3h4uLCV0QQERGRQZOc7GzYsAHR0dHo0qWLPuIhIiKif4mKok2X80mLZMfBwQE1atTQRyxERET0JHZjyUJyzhcREYFJkyYhOztbH/EQERERyUpyy07v3r2xefNmVK5cGV5eXjAzM1M7HhcXJ1twRERE5RlnY8lDcrIzYMAAxMbGol+/fhygTEREpE9cVFAWkpOd3bt3Y//+/XjzzTf1EQ8RERGRrCQnOx4eHrC3t9dHLERERPQUdkXpTvIA5blz52LcuHFISkrSQzhERESkwreey0Jyy06/fv2QlZWFmjVrwtrausQA5QcPHsgWHBERUXnGAcrykJzsLFiwQA9hEBEREemH5GQnODhYH3EQERHR07iooCy0Wkj6xo0b+Oqrr/Dhhx8iNTUVALBv3z5cunRJ1uCIiIjKs+JuLF020iLZOXr0KHx8fHDmzBlER0cjIyMDAHDhwgWEh4fLHiARERGRLiQnO59//jmmTp2KgwcPwtzcXFXu7++PU6dOyRocERFRucbZWLKQPGbn999/x6ZNm0qUV6pUCffv35clKCIiIuJsLLlIbtmpUKEC7t27V6L8/PnzqFKliixBEREREclFcrLTp08ffPbZZ0hJSYEgCFAqlfjll18wZswYfPTRR/qIkYiIqHxiN5YsJCc706ZNQ7Vq1VClShVkZGSgQYMGaNu2LVq1aoWvvvpKHzESERGVS6Ig6LyRFmN2zMzMsHHjRkyZMgVxcXFQKpVo2rQpateurY/4iIiIiHQiOdkpVqNGDdSoUUPOWIiIiOhJXFRQFlonO0RERKRfnI0lDyY7REREhootO7LQ6nURRERERK8KtuwQEREZKrbsyILJzhPSlWZQKtnYRUB1M4uyDoEMyIHU+mUdAhmQgsxcAIdfyr04ZkceWv1mt7e3x82bN0t8JiIiIjI0WrXsiKJY6mciIiKSEbuxZMFuLCIiIgPFbix5cIAKERERGTW27BARERkqdmPJgskOERGRgWI3ljzYjUVERERGjS07REREhoytMzrTKtlp06YNrKysSnwmIiIiGXHMjiy0Snb27NlT6mciIiKSD8fsyINjdoiIiMioccwOERGRoWI3liyY7BARERko8d9Nl/OJ3VhERERk5JjsEBERGSpBhk2CY8eOITAwEO7u7hAEAdu3b39m3SFDhkAQBCxYsECt3M/PD4IgqG0ffPCBtEBkplE3Vnp6usYXtLe31zoYIiIiesJLHrOTmZmJxo0bIyQkBO++++4z623fvh1nzpyBu7t7qcc//vhjTJ48WbVf1kvUaJTsVKhQAYKg2TdWWFioU0BERERUNjp37ozOnTs/t86dO3cwfPhw7N+/H127di21jrW1NVxdXfURolY0SnZiYmJUn5OSkvD5559jwIABaNmyJQDg1KlTWLt2LSIjI/UTJRERUTkk1zo7T/fQWFhYwMLCQvL1lEol+vfvj7Fjx6Jhw4bPrLdx40Zs2LABLi4u6Ny5M8LDw2FnZyf5fnLRKNlp166d6vPkyZMxb948fPjhh6qy7t27w8fHBytXrkRwcLD8URIREZVHMnVjeXh4qBWHh4cjIiJC8uVmzpwJU1NTjBgx4pl1+vbti+rVq8PV1RUXL17E+PHj8dtvv+HgwYOS7ycXyVPPT506hRUrVpQo9/X1xeDBg2UJioiIiORr2UlOTlYbU6tNq05sbCwWLlyIuLi45w5t+fjjj1Wfvb29Ubt2bfj6+iIuLg7NmjWTfF85SJ6N5eHhUWqy8/XXX5fIHImIiKjs2dvbq23aJDvHjx9HamoqqlWrBlNTU5iamuLWrVsICwuDl5fXM89r1qwZzMzMcO3aNR2eQDeSW3bmz5+Pd999F/v370eLFi0AAKdPn8aNGzewdetW2QMkIiIqtwxoBeX+/fujQ4cOamUBAQHo378/QkJCnnnepUuXkJ+fDzc3N/mCkUhystOlSxdcvXoVy5cvx5UrVyCKInr06IGhQ4eyZYeIiEhOLznZycjIwPXr11X7iYmJiI+Ph6OjI6pVqwYnJye1+mZmZnB1dUXdunUBADdu3MDGjRvRpUsXODs7448//kBYWBiaNm2K1q1b6/AgutHqdREeHh6YPn263LEQERFRGTp37hz8/f1V+6NHjwYABAcHIyoq6oXnm5ub49ChQ1i4cCEyMjLg4eGBrl27Ijw8HCYmJvoK+4U0SnYuXLgAb29vKBQKXLhw4bl1GzVqJEtgRERE5Z1cA5Q15efnB1HU/I1aSUlJavseHh44evSotJu+BBolO02aNEFKSgoqV66MJk2aQBCEUr8MQRC4qCAREZFcDGjMzqtMo2QnMTERlSpVUn0mIiIielVolOx4enqW+pmIiIj0iC07stBqgPLVq1dx5MgRpKamQqlUqh2bOHGiLIERERGVdy97zI6xkpzsrFq1Cp988gmcnZ3h6uqqtoqiIAhMdoiIiMigSE52pk6dimnTpuGzzz7TRzxERERUjN1YspCc7KSlpaF37976iIWIiIiewG4seUh+N1bv3r1x4MABfcRCRERETxJk2IxEWloa1q1bp9W5GrXsLFq0SPW5Vq1amDBhAk6fPg0fHx+YmZmp1X3ea9+JiIiItPHnn38iJCQEH330keRzNUp25s+fr7Zva2uLo0ePllglURAEJjtERERyMqLWmbKi8aKCRERE9HJxzI48JI/ZISIiInqVSJ6NVVhYiKioKBw6dKjURQUPHz4sW3BERETlWjmaev7k+ODS3LlzR+trS052Ro4ciaioKHTt2hXe3t5qiwoSERERaePp8cGlqVatmlbXlpzsbNmyBd9//z26dOmi1Q2JiIiInqbP8cGSkx1zc3PUqlVLH7EQERHRE8rbAGVRFHH9+nXk5+ejTp06MDXV6hWeJUgeoBwWFoaFCxdCFEVZAiAiIqJnKEeLCiYlJaFJkyaoV68efHx8UKtWLcTGxspybckp04kTJxATE4O9e/eiYcOGJRYVjI6OliUwIiIiKj8+++wz5OTkYP369bC0tMTs2bPxySef4Ndff9X52pKTnQoVKuCdd97R+cZERET0AuVoNtbx48exefNmtGvXDgDQvHlzeHp6Ijs7G1ZWVjpdW3Kys2bNGp1uSERERBrScczOq5TspKSkoF69eqr9qlWrwsrKCn/99Re8vLx0urY8I3+IiIhIfuWoZUcQBCgU6kOJFQqFLGOENRqg/Pbbb+PkyZMvrPf48WPMnDkTS5cu1TkwIiIiKj9EUUSdOnXg6Oio2jIyMtC0aVO1Mm1o1LLTu3dvBAUFwc7ODt27d4evry/c3d1haWmJtLQ0/PHHHzhx4gT27NmDbt26Yfbs2VoFQ0RERP8pT1PP9TlMRqNkZ9CgQejfvz9+/PFHfPfdd1i1ahUePnwIoKjZqUGDBggICEBsbCzq1q2rt2CJiIjKlXLUjRUcHKy3a2s8Zsfc3Bx9+vRBnz59AACPHj1CdnY2nJycSkw/JyIiIpLqhx9+wPbt25Gfn48OHTrg//7v/2S5rtYDlB0cHODg4CBLEERERFQKQSzadDn/FbFy5UoMHToUtWvXhqWlJbZu3YrExERERkbqfG3JKygTERHRy1E8ZkeX7VWxePFifPnll0hISMBvv/2G1atXY8mSJbJcm8kOERERlbmbN28iJCREtd+/f3/k5uYiJSVF52tznR0iIiJDVY4GKGdnZ8PW1la1b2JiAgsLC2RlZel8bSY7REREhqocJTsA8M0336glPAUFBYiKioKzs7OqbMSIEZKvq3Wyk5eXh9TUVCiVSrXyatWqaXtJIiIiKqeqVauGVatWqZW5urpi/fr1qn1BELRKdiSP2bl27RratGkDKysreHp6onr16qhevTq8vLxQvXp1yQEUi4yMhCAIGDVqlKpMFEVERETA3d0dVlZW8PPzw6VLl9TOy83NxaeffgpnZ2fY2Nige/fuuH37ttZxEBERGYryNEA5KSkJiYmJz91u3ryp1bUlt+wMGDAApqam2LVrF9zc3CAIun+TZ8+excqVK9GoUSO18lmzZmHevHmIiopCnTp1MHXqVHTs2BEJCQmws7MDAIwaNQo7d+7Eli1b4OTkhLCwMHTr1g2xsbEwMTHROTYiIqIyU866sfRFcrITHx+P2NhYtTeT6iIjIwN9+/bFqlWrMHXqVFW5KIpYsGABvvzyS/Tq1QsAsHbtWri4uGDTpk0YMmQIHj16hNWrV2P9+vXo0KEDAGDDhg3w8PDAzz//jICAAFliJCIiKhNMdmQhuRurQYMG+Oeff2QLYNiwYejatasqWSmWmJiIlJQUdOrUSVVmYWGBdu3aqV5KGhsbi/z8fLU67u7u8Pb2fu6LS3Nzc5Genq62ERERkXGSnOzMnDkT48aNw5EjR3D//n2dkoYtW7YgLi6u1NURi+fVu7i4qJW7uLiojqWkpMDc3BwVK1Z8Zp3SREZGqlaAdnBwgIeHh6S4iYiIXoriFZR12Uh6N1ZxC0z79u3VykVRhCAIKCws1Og6ycnJGDlyJA4cOABLS8tn1nt6TFDxfZ7nRXXGjx+P0aNHq/bT09OZ8BARkcERoeNbz2WL5NUmOdmJiYmR5caxsbFITU3Fa6+9piorLCzEsWPHsGTJEiQkJAAoar1xc3NT1UlNTVW19ri6uiIvLw9paWlqrTupqalo1arVM+9tYWEBCwsLWZ6DiIiI5GNiYoJ79+6hcuXKauX3799H5cqVNW5UeZLkZKddu3aSb1Ka9u3b4/fff1crCwkJQb169fDZZ5+hRo0acHV1xcGDB9G0aVMARWv7HD16FDNnzgQAvPbaazAzM8PBgwcRFBQEALh37x4uXryIWbNmyRInERERvTyiWHp7VG5uLszNzbW6plaLCj58+BCrV6/G5cuXIQgCGjRogIEDB0p6C7qdnR28vb3VymxsbODk5KQqHzVqFKZPn47atWujdu3amD59OqytrdGnTx8ARW9eHzRoEMLCwuDk5ARHR0eMGTMGPj4+JQY8ExERvXLK0WysRYsWASgavvL0SsrFPT/azgSXnOycO3cOAQEBsLKyQvPmzSGKIubNm4dp06bhwIEDaNasmVaBlGbcuHHIzs5GaGgo0tLS8MYbb+DAgQOqNXYAYP78+TA1NUVQUBCys7PRvn17REVFcY0dIiKiV8j8+fMBFLXsrFixQu33uLm5Oby8vLBixQqtri2Iz2oveoY2bdqgVq1aWLVqFUxNi3KlgoICDB48GDdv3sSxY8e0CqQspaenw8HBAUd/rwJbO74InoAmHNNFT3j7SteyDoEMSEFmLmK6rcCjR49gb2+vl3sU/17ymD0VCqtnT+J5EWV2DpLHfqXXWOXm7++P6OjoEjOtdaFVy86TiQ4AmJqaYty4cfD19ZUtMCIionJP1+njr+DU86cnQhUWFuL333+Hp6en1gmQ5GYMe3t7/PnnnyXKk5OT1bqXiIiIiKQaNWoUVq9eDaAo0Wnbti2aNWsGDw8PHDlyRKtrSk523n//fQwaNAjfffcdkpOTcfv2bWzZsgWDBw/Ghx9+qFUQREREVApBhu0V88MPP6Bx48YAgJ07dyIpKQlXrlzBqFGj8OWXX2p1TcndWHPmzIEgCPjoo49QUFAAADAzM8Mnn3yCGTNmaBUEERERlaIczcYqdv/+fbi6ugIA9uzZg969e6NOnToYNGiQasaWVJJbdszNzbFw4UKkpaUhPj4e58+fx4MHDzB//nwu1EdERCSnctiy4+Ligj/++AOFhYXYt2+faimZrKwsrWdaa7XODgBYW1vDx8dH29OJiIiISggJCUFQUBDc3NwgCAI6duwIADhz5ox+19np1asXoqKiYG9vj169ej23bnR0tFaBEBER0VPK4WysiIgIeHt7Izk5Gb1791b1GpmYmODzzz/X6poaJTsODg6qF2tKWSWZiIiIdFAOx+wAwHvvvQcAyMnJUZUFBwdrfT2Nkp01a9aU+pmIiIhIToWFhZg+fTpWrFiBv/76C1evXkWNGjUwYcIEeHl5YdCgQZKvKXmAcnZ2NrKyslT7t27dwoIFC3DgwAHJNyciIqJnEwXdt1fNtGnTEBUVhVmzZqm9+NPHxwfffPONVteUnOz06NED69atA1D0QtDmzZtj7ty56NGjB5YvX65VEERERFSK4jE7umyvmHXr1mHlypXo27ev2uyrRo0a4cqVK1pdU3KyExcXhzZt2gAAfvzxR7i6uuLWrVtYt26d1vPfiYiIiADgzp07qFWrVolypVKJ/Px8ra4pOdnJyspSvRbiwIED6NWrFxQKBVq0aIFbt25pFQQRERGVohyus9OwYUMcP368RPkPP/yApk2banVNyevs1KpVC9u3b8c777yD/fv343//+x8AIDU19ZV5oyoREdGrQBCKNl3Of1UMHDgQCxcuRHh4OPr37487d+5AqVQiOjoaCQkJWLduHXbt2qXVtSW37EycOBFjxoyBl5cX3njjDbRs2RJAUSuPthkXERERlW9r165FdnY2AgMD8d1332HPnj0QBAETJ07E5cuXsXPnTtUCg1JJbtl577338Oabb+LevXuqF3UBQPv27fHOO+9oFQQRERGVohwtKiiK/8UaEBCAgIAA2a6t1esiXF1dVS/pKta8eXNZAiIiIqJ/lbNFBQU99btJTnYyMzMxY8YMHDp0CKmpqVAqlWrHb968KVtwRERE5Vo5S3bq1KnzwoTnwYMHkq8rOdkZPHgwjh49iv79+6te0kVERESkq0mTJunltVSSk529e/di9+7daN26tezBEBER0ZPEfzddzn91fPDBB6hcubLs15Wc7FSsWBGOjo6yB0JERERPKUfdWPrsKZI89XzKlCmYOHGi2vuxiIiIiHTx5GwsuUlu2Zk7dy5u3LgBFxcXeHl5wczMTO14XFycbMERERGVZ4IgQtBh+rgu575sT094kpPkZKdnz556CIOIiIhK9Qp1RRkqyclOeHi4PuIgIiIi0gvJY3YA4OHDh/jmm28wfvx41Xz3uLg43LlzR9bgiIiIyrPibixdNtKiZefChQvo0KEDHBwckJSUhI8//hiOjo7Ytm0bbt26hXXr1ukjTiIiovKnHM3G0ifJLTujR4/GgAEDcO3aNVhaWqrKO3fujGPHjskaHBEREb08x44dQ2BgINzd3SEIArZv3/7MukOGDIEgCFiwYIFaeW5uLj799FM4OzvDxsYG3bt3x+3bt/Ub+AtITnbOnj2LIUOGlCivUqUKUlJSZAmKiIiIAEHQfZMiMzMTjRs3xpIlS55bb/v27Thz5gzc3d1LHBs1ahS2bduGLVu24MSJE8jIyEC3bt1QWFgoLRgZSe7GsrS0RHp6eonyhIQEVKpUSZagiIiICC/9reedO3dG586dn1vnzp07GD58OPbv34+uXbuqHXv06BFWr16N9evXo0OHDgCADRs2wMPDAz///LOsbzKXQnLLTo8ePTB58mTk5+cDKFrx8M8//8Tnn3+Od999V/YAiYiIyiu5Biinp6erbbm5uVrFo1Qq0b9/f4wdOxYNGzYscTw2Nhb5+fno1KmTqszd3R3e3t44efKkdl+CDCQnO3PmzMHff/+NypUrIzs7G+3atUOtWrVgZ2eHadOm6SNGIiIi0oGHhwccHBxUW2RkpFbXmTlzJkxNTTFixIhSj6ekpMDc3BwVK1ZUK3dxcSnToS6Su7Hs7e1x4sQJHD58GHFxcVAqlWjWrJmquYqIiIjkoc24m6fPB4Dk5GTY29uryi0sLCRfKzY2FgsXLkRcXJzk91iJoqjXd1+9iORkp9hbb72Ft956S85YiIiI6Em6rpXz77n29vZqyY42jh8/jtTUVFSrVk1VVlhYiLCwMCxYsABJSUlwdXVFXl4e0tLS1Fp3UlNT0apVK53urwutkp1ff/0VR44cQWpqaol3WcybN0+WwIiIiMhw9O/fv0QvTkBAAPr374+QkBAAwGuvvQYzMzMcPHgQQUFBAIB79+7h4sWLmDVr1kuPuZjkZGf69On46quvULduXbi4uKg1S5VlExUREZHRecmLCmZkZOD69euq/cTERMTHx8PR0RHVqlWDk5OTWn0zMzO4urqibt26AAAHBwcMGjQIYWFhcHJygqOjI8aMGQMfH58yHe4iOdlZuHAhvv32WwwYMEAP4RAREVGxl/3W83PnzsHf31+1P3r0aABAcHAwoqKiNLrG/PnzYWpqiqCgIGRnZ6N9+/aIioqCiYmJpFjkJDnZUSgUaN26tT5iISIiojLk5+cHUdQ8QUpKSipRZmlpicWLF2Px4sUyRqYbyVPP//e//2Hp0qX6iIWIiIieIMiwkRYtO2PGjEHXrl1Rs2ZNNGjQAGZmZmrHo6OjZQuOiIioPHvZ3VjGSnKy8+mnnyImJgb+/v5wcnLioGQiIiIyaJKTnXXr1mHr1q0l3odBRERE8ipaVFCXlh0Zg3mFSU52HB0dUbNmTX3EQkRERE+QawXl8k7yAOWIiAiEh4cjKytLH/EQERHRvxSCqPNGWrTsLFq0CDdu3ICLiwu8vLxKDFCOi4uTLTgiIiIiXUlOdnr27KmHMIiIiOhpnI0lD8nJTnh4uD7iICIioqcw2ZGH5DE7RERERK8SjVp2HB0dcfXqVTg7O6NixYrPXVvnwYMHsgVHRERUnnE2ljw0Snbmz58POzs7AMCCBQv0GQ8RERH9S9cZVSK7sQBomOwEBweX+pmIiIjI0EkeoPzo0SMcPHgQSUlJEAQBNWrUQPv27WFvb6+P+IiIiMotDlCWh6RkZ8OGDRg+fDjS09PVyh0cHLBixQq8//77sgZHRERUnikgQgEdurF0ONeYaDwbKy4uDiEhIejZsyfOnz+P7OxsZGVl4dy5cwgMDET//v3x22+/6TNWIiIiIsk0btlZvHgxevbsiaioKLXyZs2aYd26dcjKysLChQvx7bffyh0jERFRuSRAx9lYskXyatM42fnll1+wbNmyZx4fOnQoQkNDZQmqrAzcNAwmlpZlHQYZgGoRJ8s6BDIgpp5ckoz+Iypf3s+DoONsLCXH7ACQkOzcvXsXderUeebxOnXq4M6dO7IERURERBygLBeN09OsrCxYPqfVw8LCAjk5ObIERURERCQXSbOx9u/fDwcHh1KPPXz4UI54iIiI6F+6Liqoy7nGRFKy86IFBZ/3GgkiIiKSht1Y8tA42VEqlfqMg4iIiEgvJK+gTERERC8Hu7HkwWSHiIjIQOm6grIu5xoTLh5BRERERo0tO0RERAaKA5TlwWSHiIjIQHHMjjy0Tnby8vKQmppaYpZWtWrVdA6KiIiISC6Sk51r165h4MCBOHlS/d1BoihCEAQUFhbKFhwREVF5xpYdeUhOdgYMGABTU1Ps2rULbm5uXEiQiIhIT5jsyENyshMfH4/Y2FjUq1dPH/EQERHRv5jsyEPy1PMGDRrgn3/+0UcsRERERLKTnOzMnDkT48aNw5EjR3D//n2kp6erbURERCQPAf8tLKjNxoEmRSR3Y3Xo0AEA0L59e7VyDlAmIiKSF7ux5CE52YmJidFHHERERER6ITnZadeunT7iICIioqewZUceWr0b6/jx4+jXrx9atWqFO3fuAADWr1+PEydOyBocERFReVac7OiykRbJztatWxEQEAArKyvExcUhNzcXAPD48WNMnz5d9gCJiIiIdCE52Zk6dSpWrFiBVatWwczMTFXeqlUrxMXFyRocERFRecaWHXlIHrOTkJCAtm3blii3t7fHw4cP5YiJiIiI8N+0c13OJy1adtzc3HD9+vUS5SdOnECNGjVkCYqIiIhILpKTnSFDhmDkyJE4c+YMBEHA3bt3sXHjRowZMwahoaH6iJGIiKhcYjeWPCR3Y40bNw6PHj2Cv78/cnJy0LZtW1hYWGDMmDEYPny4PmIkIiIqlxSCEgpBqdP5pEWyAwDTpk3Dl19+iT/++ANKpRINGjSAra2t3LERERGVa1xnRx5aJTsAYG1tDV9fXzljISIiIpKdRslOr169NL5gdHS01sEQERHRfwQdZ2MJnI0FQMNkx8HBQfVZFEVs27YNDg4Oqpad2NhYPHz4UFJSRERERM+ngI7dWEx2AGiY7KxZs0b1+bPPPkNQUBBWrFgBExMTAEBhYSFCQ0Nhb2+vnyiJiIiItCR5zM63336LEydOqBIdADAxMcHo0aPRqlUrzJ49W9YAiYiIyivOxpKH5HV2CgoKcPny5RLlly9fhlLJL5WIiEguJoKo80ZatOyEhIRg4MCBuH79Olq0aAEAOH36NGbMmIGQkBDZAyQiIiLSheRkZ86cOXB1dcX8+fNx7949AEWvkBg3bhzCwsJkD5CIiKi84rux5CE52VEoFBg3bhzGjRuH9PR0AODAZCIiIj3gmB15aL2oIMAkh4iIiAyfRslO06ZNIQiCRheMi4vTKSAiIiIqohB0e+WDQrNf3UZPo2SnZ8+eqs85OTlYtmwZGjRogJYtWwIoGqB86dIlvvWciIhIRiYQYaLDuBtdzjUmGiU74eHhqs+DBw/GiBEjMGXKlBJ1kpOT5Y2OiIioHBN0HLMjcMwOAC3W2fnhhx/w0UcflSjv168ftm7dKktQRERERHKRnOxYWVnhxIkTJcpPnDgBS0tLWYIiIiKiovE6um6kxWysUaNG4ZNPPkFsbKzaooLffvstJk6cKHuARERE5ZWuqyBzBeUikpOdzz//HDVq1MDChQuxadMmAED9+vURFRWFoKAg2QMkIiIi0oVW6+wEBQUxsSEiItIzBZRQQIdFBXU415jotKggERER6Y+u4244ZqeIRsmOo6Mjrl69CmdnZ1SsWPG5Cww+ePBAtuCIiIiIdKVRsjN//nzY2dkBABYsWKDPeIiIiOhfJlDCRIeuKF3ONSYaJTvBwcGlfiYiIiL9YTeWPDQes1P8hvMX4ctBiYiIyJBonOxUqFDhuWN1RFGEIAgoLCyUJTAiIqLyzkRQwkSHVz7ocq4x0XgF5ZiYGBw+fBiHDx/GoUOHYGFhgfXr16vKio8TERGRPASIUOiwCRJfBHrs2DEEBgbC3d0dgiBg+/btascjIiJQr1492NjYoGLFiujQoQPOnDmjVsfPzw+CIKhtH3zwga5fhU40btlp166d2r6JiQlatGiBGjVqyB4UERERvfyWnczMTDRu3BghISF49913SxyvU6cOlixZgho1aiA7Oxvz589Hp06dcP36dVSqVElV7+OPP8bkyZNV+1ZWVlo/gxy4zg4REREBADp37ozOnTs/83ifPn3U9ufNm4fVq1fjwoULaN++varc2toarq6ueotTKskvAiUiIqKXQyEodd6AoklGT265ubk6x5aXl4eVK1fCwcEBjRs3Vju2ceNGODs7o2HDhhgzZgweP36s8/10oVPLzvMGLBMREZFuTACYSBx38/T5AODh4aFWHh4ejoiICK2uuWvXLnzwwQfIysqCm5sbDh48CGdnZ9Xxvn37onr16nB1dcXFixcxfvx4/Pbbbzh48KCWT6E7jZOdXr16qe3n5ORg6NChsLGxUSuPjo7W+OaRkZGIjo7GlStXYGVlhVatWmHmzJmoW7euqo4oipg0aRJWrlyJtLQ0vPHGG1i6dCkaNmyoqpObm4sxY8Zg8+bNyM7ORvv27bFs2TJUrVpV41iIiIiMVXJystrSMBYWFlpfy9/fH/Hx8fjnn3+watUqBAUF4cyZM6hcuTKAovE6xby9vVG7dm34+voiLi4OzZo10/4hdKBxN5aDg4Pa1q9fP7i7u5col+Lo0aMYNmwYTp8+jYMHD6KgoACdOnVCZmamqs6sWbMwb948LFmyBGfPnoWrqys6duyo1iQ2atQobNu2DVu2bMGJEyeQkZGBbt26cRo8ERG90uTqxrK3t1fbdEl2bGxsUKtWLbRo0QKrV6+GqakpVq9e/cz6zZo1g5mZGa5du6b1PXWlccvOmjVrZL/5vn37StyjcuXKiI2NRdu2bSGKIhYsWIAvv/xS1bK0du1auLi4YNOmTRgyZAgePXqE1atXY/369ejQoQMAYMOGDfDw8MDPP/+MgIAA2eMmIiJ6GUwg6tiNpf8VlEVRfO4YoEuXLiE/Px9ubm56j+VZDGqA8qNHjwAUvXgUABITE5GSkoJOnTqp6lhYWKBdu3Y4efIkACA2Nhb5+flqddzd3eHt7a2q87Tc3NwSg7WIiIjKu4yMDMTHxyM+Ph5A0e/h+Ph4/Pnnn8jMzMQXX3yB06dP49atW4iLi8PgwYNx+/Zt9O7dGwBw48YNTJ48GefOnUNSUhL27NmD3r17o2nTpmjdunWZPZfBTD0XRRGjR4/Gm2++CW9vbwBASkoKAMDFxUWtrouLC27duqWqY25ujooVK5aoU3z+0yIjIzFp0iS5H4GIiEhWT3ZFaXu+FOfOnYO/v79qf/To0QCK3ou5YsUKXLlyBWvXrsU///wDJycnvP766zh+/LhqHK25uTkOHTqEhQsXIiMjAx4eHujatSvCw8NhYmJS6j1fBoNJdoYPH44LFy7gxIkTJY49Peur+NUUz/O8OuPHj1f9DwSKpuQ9PVKdiIiorCl0fOu5QuK5fn5+EMVnd329aBKSh4cHjh49KumeL4NBdGN9+umn2LFjB2JiYtRmUBUvSPR0C01qaqqqtcfV1RV5eXlIS0t7Zp2nWVhYlBisRURERMapTJMdURQxfPhwREdH4/Dhw6hevbra8eJ5+k/Ozc/Ly8PRo0fRqlUrAMBrr70GMzMztTr37t3DxYsXVXWIiIheRQpB1HmjMu7GGjZsGDZt2oSffvoJdnZ2qhYcBwcHWFlZQRAEjBo1CtOnT0ft2rVRu3ZtTJ8+HdbW1qolqx0cHDBo0CCEhYXByckJjo6OGDNmDHx8fFSzs4iIiF5FJjp2Y+lyrjEp02Rn+fLlAIr6CJ+0Zs0aDBgwAAAwbtw4ZGdnIzQ0VLWo4IEDB2BnZ6eqP3/+fJiamiIoKEi1qGBUVFSZDoYiIiLS1ct+EaixKtNk53mDoIoJgoCIiIjnLmttaWmJxYsXY/HixTJGR0RERMbAYGZjERERkToFRCh0WBhQl3ONCZMdIiIiA8VuLHkYxNRzIiIiIn1hyw4REZGBUkApeWHAp88nJjtEREQGSwHARIe1cth9U4TfAxERERk1tuwQEREZqKJFBZ//LsgXnU9MdoiIiAxW0VvPtU92dHljujFhNxYREREZNbbsEBERGSh2Y8mDyQ4REZGBMhFEnWZj6XKuMWGyQ0REZKCK1tnRYcwOW3YAcMwOERERGTm27BARERmoondj6TBmh7OxADDZISIiMlgKHQcosxurCLuxiIiIyKixZYeIiMhAKSBCAV3ejcXZWACTHSIiIoPFMTvyYDcWERERGTW27BARERkoE4gw0aErSpdzjQmTHSIiIgNVNGZH+64ojtkpwm4sIiIiMmps2SEiIjJQRQOUdTufmOwQEREZLI7ZkQeTHSIiIgMlCCIUOry5XOBbzwFwzA4REREZObbsEBERGSgTKGGi4/nEZIeIiMhgccyOPNiNRUREREaNLTtEREQGSqHjAGVdzjUmTHaIiIgMFLux5MFuLCIiIjJqbNkhIiIyUGzZkQeTHSIiIgOlEIo2Xc4ndmMRERGRkWPLDhERkYFS6NiNpWA3FgAmO0RERAZLAd26YNh9U4TJDhERkYEyEYo2Xc4nJn1ERERk5NiyQ0REZKBMIMAE2jfP6HKuMWGyQ0REZKA4Zkce/B6IiIjIqLFlh4iIyECZCAJMBB26sXQ415gw2SEiIjJQCghQ6DDuRpdzjQm7sYiIiMiosWWHiIjIQCl0nI3Flp0iTHaIiIgMFLux5MFuLCIiIjJqbNkhIiIyUJyNJQ8mO0RERAZK8e9/2p9PAJMdIiIig8UxO/Jg0kdERERGjS07REREBspEUMBE0L5dwoQNOwCY7BARERmsom4sXcbsiDJG8+pisgNAFIt+GJS5OWUcCRmKAjG/rEMgQ6LMLesIyIAUKPMA/Pe7Q5/SHyvL9HxjIYgv4/+Wgbt9+zY8PDzKOgwiInqFJCcno2rVqnq5dk5ODqpXr46UlBSdr+Xq6orExERYWlrKENmrickOAKVSibt378LOzg5COV6TID09HR4eHkhOToa9vX1Zh0NljD8P9DT+TBQRRRGPHz+Gu7s7FAr9zfPJyclBXl6eztcxNzcv14kOwG4sAIBCodBbdv4qsre3L9d/kZE6/jzQ0/gzATg4OOj9HpaWluU+SZELp54TERGRUWOyQ0REREaNyQ6pWFhYIDw8HBYWFmUdChkA/jzQ0/gzQa8qDlAmIiIio8aWHSIiIjJqTHaIiIjIqDHZISIiIqPGZIeIiIiMGpMdwrFjxxAYGAh3d3cIgoDt27eXdUhUhiIjI/H666/Dzs4OlStXRs+ePZGQkFDWYVEZWb58ORo1aqRaSLBly5bYu3dvWYdFJAmTHUJmZiYaN26MJUuWlHUoZACOHj2KYcOG4fTp0zh48CAKCgrQqVMnZGZmlnVoVAaqVq2KGTNm4Ny5czh37hzeeust9OjRA5cuXSrr0Ig0xqnnpEYQBGzbtg09e/Ys61DIQPz999+oXLkyjh49irZt25Z1OGQAHB0dMXv2bAwaNKisQyHSCN+NRUTP9ejRIwBFv+CofCssLMQPP/yAzMxMtGzZsqzDIdIYkx0ieiZRFDF69Gi8+eab8Pb2LutwqIz8/vvvaNmyJXJycmBra4tt27ahQYMGZR0WkcaY7BDRMw0fPhwXLlzAiRMnyjoUKkN169ZFfHw8Hj58iK1btyI4OBhHjx5lwkOvDCY7RFSqTz/9FDt27MCxY8dQtWrVsg6HypC5uTlq1aoFAPD19cXZs2excOFCfP3112UcGZFmmOwQkRpRFPHpp59i27ZtOHLkCKpXr17WIZGBEUURubm5ZR0GkcaY7BAyMjJw/fp11X5iYiLi4+Ph6OiIatWqlWFkVBaGDRuGTZs24aeffoKdnR1SUlIAAA4ODrCysirj6Ohl++KLL9C5c2d4eHjg8ePH2LJlC44cOYJ9+/aVdWhEGuPUc8KRI0fg7+9fojw4OBhRUVEvPyAqU4IglFq+Zs0aDBgw4OUGQ2Vu0KBBOHToEO7duwcHBwc0atQIn332GTp27FjWoRFpjMkOERERGTWuoExERERGjckOERERGTUmO0RERGTUmOwQERGRUWOyQ0REREaNyQ4REREZNSY7REREZNSY7BC9IpKSkiAIAuLj48s6FJUrV66gRYsWsLS0RJMmTco6HCKiUjHZIdLQgAEDIAgCZsyYoVa+ffv2Z646bOzCw8NhY2ODhIQEHDp0qNQ6AwYMQM+ePV9uYERET2CyQySBpaUlZs6cibS0tLIORTZ5eXlan3vjxg28+eab8PT0hJOTk4xR6Zcuz0xErx4mO0QSdOjQAa6uroiMjHxmnYiIiBJdOgsWLICXl5dqv7i1Y/r06XBxcUGFChUwadIkFBQUYOzYsXB0dETVqlXx7bfflrj+lStX0KpVK1haWqJhw4Y4cuSI2vE//vgDXbp0ga2tLVxcXNC/f3/8888/quN+fn4YPnw4Ro8eDWdn52e+40ipVGLy5MmoWrUqLCws0KRJE7WXPwqCgNjYWEyePBmCICAiIuLZX9xzzJs3Dz4+PrCxsYGHhwdCQ0ORkZEBAMjMzIS9vT1+/PFHtXN27twJGxsbPH78GABw584dvP/++6hYsSKcnJzQo0cPJCUlqeoXf9+RkZFwd3dHnTp1AADLli1D7dq1YWlpCRcXF7z33ntaPQMRGTYmO0QSmJiYYPr06Vi8eDFu376t07UOHz6Mu3fv4tixY5g3bx4iIiLQrVs3VKxYEWfOnMHQoUMxdOhQJCcnq503duxYhIWF4fz582jVqhW6d++O+/fvAwDu3buHdu3aoUmTJjh37hz27duHv/76C0FBQWrXWLt2LUxNTfHLL7/g66+/LjW+hQsXYu7cuZgzZw4uXLiAgIAAdO/eHdeuXVPdq2HDhggLC8O9e/cwZswYrb4HhUKBRYsW4eLFi1i7di0OHz6McePGAQBsbGzwwQcfYM2aNWrnrFmzBu+99x7s7OyQlZUFf39/2Nra4tixYzhx4gRsbW3x9ttvq7XgHDp0CJcvX8bBgwexa9cunDt3DiNGjMDkyZORkJCAffv2oW3btlo9AxEZOJGINBIcHCz26NFDFEVRbNGihThw4EBRFEVx27Zt4pN/lMLDw8XGjRurnTt//nzR09NT7Vqenp5iYWGhqqxu3bpimzZtVPsFBQWijY2NuHnzZlEURTExMVEEIM6YMUNVJz8/X6xatao4c+ZMURRFccKECWKnTp3U7p2cnCwCEBMSEkRRFMV27dqJTZo0eeHzuru7i9OmTVMre/3118XQ0FDVfuPGjcXw8PDnXufJ700T33//vejk5KTaP3PmjGhiYiLeuXNHFEVR/Pvvv0UzMzPxyJEjoiiK4urVq8W6deuKSqVSdU5ubq5oZWUl7t+/XxWDi4uLmJubq6qzdetW0d7eXkxPT9c4NiJ6NbFlh0gLM2fOxNq1a/HHH39ofY2GDRtCofjvj6CLiwt8fHxU+yYmJnByckJqaqraeS1btlR9NjU1ha+vLy5fvgwAiI2NRUxMDGxtbVVbvXr1ABSNrynm6+v73NjS09Nx9+5dtG7dWq28devWqnvJJSYmBh07dkSVKlVgZ2eHjz76CPfv30dmZiYAoHnz5mjYsCHWrVsHAFi/fj2qVaumaoWJjY3F9evXYWdnp3pmR0dH5OTkqD2zj48PzM3NVfsdO3aEp6cnatSogf79+2Pjxo3IysqS9dmIyDAw2SHSQtu2bREQEIAvvviixDGFQgFRFNXK8vPzS9QzMzNT2xcEodQypVL5wniKZ4MplUoEBgYiPj5ebbt27ZpaF42Njc0Lr/nkdYuJoijrzLNbt26hS5cu8Pb2xtatWxEbG4ulS5cCUP/OBg8erOrKWrNmDUJCQtSe+bXXXivxzFevXkWfPn1U13j6me3s7BAXF4fNmzfDzc0NEydOROPGjfHw4UPZno+IDAOTHSItzZgxAzt37sTJkyfVyitVqoSUlBS1hEfOtXFOnz6t+lxQUIDY2FhV602zZs1w6dIleHl5oVatWmqbpgkOANjb28Pd3R0nTpxQKz958iTq168vz4MAOHfuHAoKCjB37ly0aNECderUwd27d0vU69evH/78808sWrQIly5dQnBwsOpYs2bNcO3aNVSuXLnEMzs4ODz3/qampujQoQNmzZqFCxcuICkpCYcPH5bt+YjIMDDZIdKSj48P+vbti8WLF6uV+/n54e+//8asWbNw48YNLF26FHv37pXtvkuXLsW2bdtw5coVDBs2DGlpaRg4cCAAYNiwYXjw4AE+/PBD/Prrr7h58yYOHDiAgQMHorCwUNJ9xo4di5kzZ+K7775DQkICPv/8c8THx2PkyJGSY3706FGJlpc///wTNWvWREFBARYvXoybN29i/fr1WLFiRYnzK1asiF69emHs2LHo1KkTqlatqjrWt29fODs7o0ePHjh+/DgSExNx9OhRjBw58rmDyHft2oVFixYhPj4et27dwrp166BUKlG3bl3Jz0dEho3JDpEOpkyZUqLLqn79+li2bBmWLl2Kxo0b49dff9V6plJpZsyYgZkzZ6Jx48Y4fvw4fvrpJzg7OwMA3N3d8csvv6CwsBABAQHw9vbGyJEj4eDgoDY+SBMjRoxAWFgYwsLC4OPjg3379mHHjh2oXbu25JiPHDmCpk2bqm0TJ05EkyZNMG/ePMycORPe3t7YuHHjM6f1Dxo0CHl5earErpi1tTWOHTuGatWqoVevXqhfvz4GDhyI7Oxs2NvbPzOmChUqIDo6Gm+99Rbq16+PFStWYPPmzWjYsKHk5yMiwyaIT/9NTURkgDZu3IiRI0fi7t27agONiYhexLSsAyAiep6srCwkJiYiMjISQ4YMYaJDRJKxG4uIDNqsWbPQpEkTuLi4YPz48WUdDhG9gtiNRUREREaNLTtERERk1JjsEBERkVFjskNERERGjckOERERGTUmO0RERGTUmOwQERGRUWOyQ0REREaNyQ4REREZNSY7REREZNT+Hzoh9kLo5XRvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract unique nhid and nlayers values\n",
    "nhid_values = sorted(set(key[0] for key in results.keys()))\n",
    "nlayers_values = sorted(set(key[1] for key in results.keys()))\n",
    "\n",
    "# Create a 2D array for Z values\n",
    "Z = np.full((len(nhid_values), len(nlayers_values)), np.nan)  # Initialize with NaN\n",
    "\n",
    "# Populate the 2D array with PPL values\n",
    "for (nhid, nlayers), ppl in results.items():\n",
    "    i = nhid_values.index(nhid)\n",
    "    j = nlayers_values.index(nlayers)\n",
    "    Z[i, j] = ppl\n",
    "\n",
    "# Plot the results as a heatmap\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.imshow(Z, cmap=\"viridis_r\", aspect=\"auto\")\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(cax)\n",
    "cbar.set_label('Test PPL')\n",
    "\n",
    "# Set tick positions and labels for the y-axis (nhid values)\n",
    "ax.set_yticks(np.arange(len(nhid_values)))\n",
    "ax.set_yticklabels(nhid_values)\n",
    "ax.set_ylabel('Hidden Dimension (nhid = emsize)')\n",
    "\n",
    "# Set tick positions and labels for the x-axis (nlayers values)\n",
    "ax.set_xticks(np.arange(len(nlayers_values)))\n",
    "ax.set_xticklabels(nlayers_values)\n",
    "ax.set_xlabel('Number of Layers')\n",
    "\n",
    "# Set the title\n",
    "plt.title('Test PPL by nhid and nlayers')\n",
    "\n",
    "# Invert the y-axis to have the larger nhid values at the top\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot represents a heatmap representing test Perplexity (PPL) across different configurations of hidden layer dimensions (`nhid`) and the number of layers (`nlayers`). \n",
    "\n",
    "1. Color Coding: The heatmap uses color to represent the magnitude of test PPL, where each cell corresponds to a combination of `nhid` and `nlayers`. A darker color (towards yellow) indicates lower PPL, which is generally better, as it suggests the model is more confident in its predictions. Conversely, darker colors (towards purple) indicate higher PPL.\n",
    "\n",
    "2. Hidden Dimensions:\n",
    "   - As the hidden dimension size increases from 200 to 600, there does not appear to be a consistent trend in the test PPL. This suggests that simply increasing the capacity of the model (in terms of hidden units) does not linearly relate to better performance.\n",
    "   - The best PPL is achieved with a hidden dimension size of 400 units. This suggests that there may be an optimal size for the hidden dimension that balances model complexity with the ability to generalize from the training data.\n",
    "\n",
    "3. Number of Layers:\n",
    "   - The number of layers appears to have a more direct impact on the PPL, with 2 layers consistently outperforming 1 and 3 layers across all hidden dimension sizes. This may indicate that adding layers does not necessarily improve performance and could lead to overfitting or difficulties in optimization.\n",
    "   - The increase in PPL from 1 to 3 layers could also suggest that the additional complexity introduced by more layers does not help and may hinder the model given the fixed size of the dataset.\n",
    "\n",
    "4. Optimal Configuration:\n",
    "   - The optimal configuration within the tested parameters seems to be with a hidden dimension of 400 and 2 layers, as indicated by the lightest color in the heatmap.\n",
    "\n",
    "5. Overfitting and Model Complexity:\n",
    "   - It’s possible that larger models (with higher `nhid` or more `nlayers`) could overfit the training data, especially if the dataset is not large enough to support the increased capacity, resulting in higher test PPL.\n",
    "   - It is also plausible that the optimal point represents the best trade-off between model complexity and its capacity to generalize, indicating that further complexity (more layers or larger hidden dimensions) does not contribute to better performance on the test set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
